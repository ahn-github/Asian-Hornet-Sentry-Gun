ccopy_reg
_reconstructor
p0
(cdigits.model.images.generic.job
GenericImageModelJob
p1
c__builtin__
object
p2
Ntp3
Rp4
(dp5
S'username'
p6
Vnvidia
p7
sS'_notes'
p8
NsS'tasks'
p9
(lp10
g0
(cdigits.model.tasks.caffe_train
CaffeTrainTask
p11
g2
Ntp12
Rp13
(dp14
S'shuffle'
p15
I01
sS'snapshot_interval'
p16
F10.0
sS'train_outputs'
p17
ccollections
OrderedDict
p18
((lp19
(lp20
S'epoch'
p21
ag0
(cdigits.model.tasks.train
NetworkOutput
p22
c__builtin__
tuple
p23
(S'Epoch'
p24
(lp25
I0
aF0.002785515320334262
aF0.005571030640668524
aF0.12256267409470752
aF0.24512534818941503
aF0.36768802228412256
aF0.49025069637883006
aF0.6128133704735376
aF0.7353760445682451
aF0.8579387186629527
aF0.9805013927576601
aF1.1030640668523677
aF1.2256267409470751
aF1.3481894150417828
aF1.4707520891364902
aF1.5933147632311977
aF1.7158774373259054
aF1.8384401114206128
aF1.9610027855153203
aF2.083565459610028
aF2.2061281337047354
aF2.328690807799443
aF2.4512534818941503
aF2.573816155988858
aF2.6963788300835656
aF2.818941504178273
aF2.9415041782729805
aF3.064066852367688
aF3.1866295264623954
aF3.3091922005571033
aF3.4317548746518107
aF3.554317548746518
aF3.6768802228412256
aF3.799442896935933
aF3.9220055710306405
aF4.044568245125348
aF4.167130919220056
aF4.289693593314763
aF4.412256267409471
aF4.534818941504178
aF4.657381615598886
aF4.779944289693593
aF4.9025069637883005
aF5.025069637883008
aF5.147632311977716
aF5.270194986072424
aF5.392757660167131
aF5.515320334261839
aF5.637883008356546
aF5.7604456824512535
aF5.883008356545961
aF6.005571030640668
aF6.128133704735376
aF6.250696378830083
aF6.373259052924791
aF6.495821727019498
aF6.618384401114207
aF6.740947075208914
aF6.8635097493036215
aF6.986072423398329
aF7.108635097493036
aF7.231197771587744
aF7.353760445682451
aF7.476323119777159
aF7.598885793871866
aF7.721448467966574
aF7.844011142061281
aF7.9665738161559885
aF8.089136490250697
aF8.211699164345404
aF8.334261838440112
aF8.45682451253482
aF8.579387186629527
aF8.701949860724234
aF8.824512534818941
aF8.947075208913649
aF9.069637883008356
aF9.192200557103064
aF9.314763231197771
aF9.437325905292479
aF9.559888579387186
aF9.682451253481894
aF9.805013927576601
aF9.927576601671309
aF10.050139275766016
aF10.172701949860723
aF10.295264623955433
aF10.41782729805014
aF10.540389972144848
aF10.662952646239555
aF10.785515320334262
aF10.90807799442897
aF11.030640668523677
aF11.153203342618385
aF11.275766016713092
aF11.3983286908078
aF11.520891364902507
aF11.643454038997215
aF11.766016713091922
aF11.88857938718663
aF12.011142061281337
aF12.133704735376044
aF12.256267409470752
aF12.37883008356546
aF12.501392757660167
aF12.623955431754874
aF12.746518105849582
aF12.869080779944289
aF12.991643454038996
aF13.114206128133704
aF13.236768802228413
aF13.35933147632312
aF13.481894150417828
aF13.604456824512535
aF13.727019498607243
aF13.84958217270195
aF13.972144846796658
aF14.094707520891365
aF14.217270194986073
aF14.33983286908078
aF14.462395543175488
aF14.584958217270195
aF14.707520891364902
aF14.83008356545961
aF14.952646239554317
aF15.075208913649025
aF15.197771587743732
aF15.32033426183844
aF15.442896935933147
aF15.565459610027855
aF15.688022284122562
aF15.81058495821727
aF15.933147632311977
aF16.055710306406684
aF16.178272980501394
aF16.3008356545961
aF16.42339832869081
aF16.545961002785514
aF16.668523676880223
aF16.79108635097493
aF16.91364902506964
aF17.036211699164344
aF17.158774373259053
aF17.28133704735376
aF17.403899721448468
aF17.526462395543174
aF17.649025069637883
aF17.771587743732592
aF17.894150417827298
aF18.016713091922007
aF18.139275766016713
aF18.261838440111422
aF18.384401114206128
aF18.506963788300837
aF18.629526462395543
aF18.75208913649025
aF18.874651810584957
aF18.997214484679667
aF19.119777158774372
aF19.24233983286908
aF19.364902506963787
aF19.487465181058496
aF19.610027855153202
aF19.73259052924791
aF19.855153203342617
aF19.977715877437326
aF20.100278551532032
aF20.22284122562674
aF20.345403899721447
aF20.467966573816156
aF20.590529247910865
aF20.71309192200557
aF20.83565459610028
aF20.958217270194986
aF21.080779944289695
aF21.2033426183844
aF21.32590529247911
aF21.448467966573816
aF21.571030640668525
aF21.69359331476323
aF21.81615598885794
aF21.938718662952645
aF22.061281337047355
aF22.18384401114206
aF22.30640668523677
aF22.428969359331475
aF22.551532033426184
aF22.67409470752089
aF22.7966573816156
aF22.919220055710305
aF23.041782729805014
aF23.16434540389972
aF23.28690807799443
aF23.409470752089135
aF23.532033426183844
aF23.654596100278553
aF23.77715877437326
aF23.899721448467968
aF24.022284122562674
aF24.144846796657383
aF24.26740947075209
aF24.389972144846798
aF24.512534818941504
aF24.635097493036213
aF24.75766016713092
aF24.880222841225628
aF25.002785515320333
aF25.125348189415043
aF25.24791086350975
aF25.370473537604457
aF25.493036211699163
aF25.615598885793872
aF25.738161559888578
aF25.860724233983287
aF25.983286908077993
aF26.105849582172702
aF26.228412256267408
aF26.350974930362117
aF26.473537604456826
aF26.596100278551532
aF26.71866295264624
aF26.841225626740947
aF26.963788300835656
aF27.08635097493036
aF27.20891364902507
aF27.331476323119777
aF27.454038997214486
aF27.57660167130919
aF27.6991643454039
aF27.821727019498606
aF27.944289693593316
aF28.06685236768802
aF28.18941504178273
aF28.311977715877436
aF28.434540389972145
aF28.55710306406685
aF28.67966573816156
aF28.802228412256266
aF28.924791086350975
aF29.04735376044568
aF29.16991643454039
aF29.292479108635096
aF29.415041782729805
aF29.537604456824514
aF29.66016713091922
aF29.78272980501393
aF29.905292479108635
aF30.027855153203344
aF30.15041782729805
aF30.27298050139276
aF30.395543175487465
aF30.518105849582174
aF30.64066852367688
aF30.76323119777159
aF30.885793871866294
aF31.008356545961004
aF31.13091922005571
aF31.25348189415042
aF31.376044568245124
aF31.498607242339833
aF31.62116991643454
aF31.74373259052925
aF31.866295264623954
aF31.988857938718663
aF32.11142061281337
aF32.23398328690808
aF32.35654596100279
aF32.4791086350975
aF32.6016713091922
aF32.72423398328691
aF32.84679665738162
aF32.969359331476326
aF33.09192200557103
aF33.21448467966574
aF33.33704735376045
aF33.459610027855156
aF33.58217270194986
aF33.70473537604457
aF33.82729805013928
aF33.949860724233986
aF34.07242339832869
aF34.1949860724234
aF34.317548746518106
aF34.440111420612816
aF34.56267409470752
aF34.68523676880223
aF34.807799442896936
aF34.930362116991645
aF35.05292479108635
aF35.17548746518106
aF35.298050139275766
aF35.420612813370475
aF35.543175487465184
aF35.66573816155989
aF35.788300835654596
aF35.910863509749305
aF36.033426183844014
aF36.155988857938716
aF36.278551532033426
aF36.401114206128135
aF36.523676880222844
aF36.646239554317546
aF36.768802228412255
aF36.891364902506965
aF37.013927576601674
aF37.136490250696376
aF37.259052924791085
aF37.381615598885794
aF37.5041782729805
aF37.626740947075206
aF37.749303621169915
aF37.871866295264624
aF37.99442896935933
aF38.116991643454035
aF38.239554317548745
aF38.362116991643454
aF38.48467966573816
aF38.60724233983287
aF38.729805013927574
aF38.852367688022284
aF38.97493036211699
aF39.0974930362117
aF39.220055710306404
aF39.34261838440111
aF39.46518105849582
aF39.58774373259053
aF39.710306406685234
aF39.83286908077994
aF39.95543175487465
aF40.07799442896936
aF40.200557103064064
aF40.32311977715877
aF40.44568245125348
aF40.56824512534819
aF40.690807799442894
aF40.8133704735376
aF40.93593314763231
aF41.05849582172702
aF41.18105849582173
aF41.30362116991643
aF41.42618384401114
aF41.54874651810585
aF41.67130919220056
aF41.79387186629526
aF41.91643454038997
aF42.03899721448468
aF42.16155988857939
aF42.28412256267409
aF42.4066852367688
aF42.52924791086351
aF42.65181058495822
aF42.77437325905292
aF42.89693593314763
aF43.01949860724234
aF43.14206128133705
aF43.26462395543175
aF43.38718662952646
aF43.50974930362117
aF43.63231197771588
aF43.75487465181058
aF43.87743732590529
aF44.0
aF44.12256267409471
aF44.24512534818942
aF44.36768802228412
aF44.49025069637883
aF44.61281337047354
aF44.73537604456825
aF44.85793871866295
aF44.98050139275766
aF45.10306406685237
aF45.22562674094708
aF45.34818941504178
aF45.47075208913649
aF45.5933147632312
aF45.71587743732591
aF45.83844011142061
aF45.96100278551532
aF46.08356545961003
aF46.20612813370474
aF46.32869080779944
aF46.45125348189415
aF46.57381615598886
aF46.69637883008357
aF46.81894150417827
aF46.94150417827298
aF47.06406685236769
aF47.1866295264624
aF47.309192200557106
aF47.43175487465181
aF47.55431754874652
aF47.67688022284123
aF47.799442896935936
aF47.92200557103064
aF48.04456824512535
aF48.16713091922006
aF48.289693593314766
aF48.41225626740947
aF48.53481894150418
aF48.65738161559889
aF48.779944289693596
aF48.9025069637883
aF49.02506963788301
aF49.147632311977716
aF49.270194986072426
aF49.39275766016713
aF49.51532033426184
aF49.637883008356546
aF49.760445682451255
aF49.883008356545965
aF50.00557103064067
aF50.128133704735376
aF50.250696378830085
aF50.373259052924794
aF50.4958217270195
aF50.618384401114206
aF50.740947075208915
aF50.863509749303624
aF50.986072423398326
aF51.108635097493035
aF51.231197771587745
aF51.353760445682454
aF51.476323119777156
aF51.598885793871865
aF51.721448467966574
aF51.844011142061284
aF51.966573816155986
aF52.089136490250695
aF52.211699164345404
aF52.33426183844011
aF52.456824512534816
aF52.579387186629525
aF52.701949860724234
aF52.82451253481894
aF52.94707520891365
aF53.069637883008355
aF53.192200557103064
aF53.31476323119777
aF53.43732590529248
aF53.559888579387184
aF53.682451253481894
aF53.8050139275766
aF53.92757660167131
aF54.050139275766014
aF54.17270194986072
aF54.29526462395543
aF54.41782729805014
aF54.540389972144844
aF54.66295264623955
aF54.78551532033426
aF54.90807799442897
aF55.030640668523674
aF55.15320334261838
aF55.27576601671309
aF55.3983286908078
aF55.5208913649025
aF55.64345403899721
aF55.76601671309192
aF55.88857938718663
aF56.01114206128134
aF56.13370473537604
aF56.25626740947075
aF56.37883008356546
aF56.50139275766017
aF56.62395543175487
aF56.74651810584958
aF56.86908077994429
aF56.991643454039
aF57.1142061281337
aF57.23676880222841
aF57.35933147632312
aF57.48189415041783
aF57.60445682451253
aF57.72701949860724
aF57.84958217270195
aF57.97214484679666
aF58.09470752089136
aF58.21727019498607
aF58.33983286908078
aF58.46239554317549
aF58.58495821727019
aF58.7075208913649
aF58.83008356545961
aF58.95264623955432
aF59.07520891364903
aF59.19777158774373
aF59.32033426183844
aF59.44289693593315
aF59.56545961002786
aF59.68802228412256
aF59.81058495821727
aF59.93314763231198
aF60.05571030640669
aF60.17827298050139
aF60.3008356545961
aF60.42339832869081
aF60.54596100278552
aF60.66852367688022
aF60.79108635097493
aF60.91364902506964
aF61.03621169916435
aF61.15877437325905
aF61.28133704735376
aF61.40389972144847
aF61.52646239554318
aF61.64902506963789
aF61.77158774373259
aF61.8941504178273
aF62.01671309192201
aF62.139275766016716
aF62.26183844011142
aF62.38440111420613
aF62.50696378830084
aF62.629526462395546
aF62.75208913649025
aF62.87465181058496
aF62.99721448467967
aF63.119777158774376
aF63.24233983286908
aF63.36490250696379
aF63.4874651810585
aF63.610027855153206
aF63.73259052924791
aF63.85515320334262
aF63.977715877437326
aF64.10027855153203
aF64.22284122562674
aF64.34540389972145
aF64.46796657381616
aF64.59052924791087
aF64.71309192200557
aF64.83565459610028
aF64.958217270195
aF65.08077994428969
aF65.2033426183844
aF65.3259052924791
aF65.44846796657382
aF65.57103064066852
aF65.69359331476323
aF65.81615598885794
aF65.93871866295265
aF66.06128133704735
aF66.18384401114206
aF66.30640668523677
aF66.42896935933148
aF66.55153203342618
aF66.6740947075209
aF66.7966573816156
aF66.91922005571031
aF67.041782729805
aF67.16434540389972
aF67.28690807799443
aF67.40947075208913
aF67.53203342618384
aF67.65459610027855
aF67.77715877437326
aF67.89972144846797
aF68.02228412256268
aF68.14484679665738
aF68.26740947075209
aF68.3899721448468
aF68.5125348189415
aF68.63509749303621
aF68.75766016713092
aF68.88022284122563
aF69.00278551532034
aF69.12534818941504
aF69.24791086350974
aF69.37047353760445
aF69.49303621169916
aF69.61559888579387
aF69.73816155988858
aF69.86072423398329
aF69.983286908078
aF70.1058495821727
aF70.2284122562674
aF70.35097493036211
aF70.47353760445682
aF70.59610027855153
aF70.71866295264624
aF70.84122562674095
aF70.96378830083566
aF71.08635097493037
aF71.20891364902506
aF71.33147632311977
aF71.45403899721448
aF71.57660167130919
aF71.6991643454039
aF71.82172701949861
aF71.94428969359332
aF72.06685236768803
aF72.18941504178272
aF72.31197771587743
aF72.43454038997214
aF72.55710306406685
aF72.67966573816156
aF72.80222841225627
aF72.92479108635098
aF73.04735376044569
aF73.16991643454038
aF73.29247910863509
aF73.4150417827298
aF73.53760445682451
aF73.66016713091922
aF73.78272980501393
aF73.90529247910864
aF74.02785515320335
aF74.15041782729806
aF74.27298050139275
aF74.39554317548746
aF74.51810584958217
aF74.64066852367688
aF74.76323119777159
aF74.8857938718663
aF75.008356545961
aF75.13091922005572
aF75.25348189415041
aF75.37604456824512
aF75.49860724233983
aF75.62116991643454
aF75.74373259052925
aF75.86629526462396
aF75.98885793871867
aF76.11142061281338
aF76.23398328690807
aF76.35654596100278
aF76.47910863509749
aF76.6016713091922
aF76.72423398328691
aF76.84679665738162
aF76.96935933147633
aF77.09192200557104
aF77.21448467966574
aF77.33704735376044
aF77.45961002785515
aF77.58217270194986
aF77.70473537604457
aF77.82729805013928
aF77.94986072423399
aF78.0724233983287
aF78.1949860724234
aF78.3175487465181
aF78.44011142061281
aF78.56267409470752
aF78.68523676880223
aF78.80779944289694
aF78.93036211699165
aF79.05292479108635
aF79.17548746518106
aF79.29805013927577
aF79.42061281337047
aF79.54317548746518
aF79.66573816155989
aF79.7883008356546
aF79.9108635097493
aF80.03342618384401
aF80.15598885793872
aF80.27855153203343
aF80.40111420612813
aF80.52367688022284
aF80.64623955431755
aF80.76880222841226
aF80.89136490250696
aF81.01392757660167
aF81.13649025069638
aF81.25905292479109
aF81.38161559888579
aF81.5041782729805
aF81.6267409470752
aF81.74930362116991
aF81.87186629526462
aF81.99442896935933
aF82.11699164345404
aF82.23955431754875
aF82.36211699164346
aF82.48467966573816
aF82.60724233983287
aF82.72980501392757
aF82.85236768802228
aF82.974930362117
aF83.0974930362117
aF83.22005571030641
aF83.34261838440112
aF83.46518105849582
aF83.58774373259052
aF83.71030640668523
aF83.83286908077994
aF83.95543175487465
aF84.07799442896936
aF84.20055710306407
aF84.32311977715878
aF84.44568245125348
aF84.56824512534818
aF84.6908077994429
aF84.8133704735376
aF84.93593314763231
aF85.05849582172702
aF85.18105849582173
aF85.30362116991644
aF85.42618384401115
aF85.54874651810584
aF85.67130919220055
aF85.79387186629526
aF85.91643454038997
aF86.03899721448468
aF86.16155988857939
aF86.2841225626741
aF86.40668523676881
aF86.5292479108635
aF86.65181058495821
aF86.77437325905292
aF86.89693593314763
aF87.01949860724234
aF87.14206128133705
aF87.26462395543176
aF87.38718662952647
aF87.50974930362116
aF87.63231197771587
aF87.75487465181058
aF87.87743732590529
aF88.0
aF88.12256267409471
aF88.24512534818942
aF88.36768802228413
aF88.49025069637884
aF88.61281337047353
aF88.73537604456824
aF88.85793871866295
aF88.98050139275766
aF89.10306406685237
aF89.22562674094708
aF89.34818941504179
aF89.4707520891365
aF89.59331476323119
aF89.7158774373259
aF89.83844011142061
aF89.96100278551532
aF90.08356545961003
aF90.20612813370474
aF90.32869080779945
aF90.45125348189416
aF90.57381615598885
aF90.69637883008356
aF90.81894150417827
aF90.94150417827298
aF91.06406685236769
aF91.1866295264624
aF91.3091922005571
aF91.43175487465182
aF91.55431754874652
aF91.67688022284122
aF91.79944289693593
aF91.92200557103064
aF92.04456824512535
aF92.16713091922006
aF92.28969359331477
aF92.41225626740948
aF92.53481894150418
aF92.65738161559888
aF92.77994428969359
aF92.9025069637883
aF93.025069637883
aF93.14763231197772
aF93.27019498607243
aF93.39275766016713
aF93.51532033426184
aF93.63788300835654
aF93.76044568245125
aF93.88300835654596
aF94.00557103064067
aF94.12813370473538
aF94.25069637883009
aF94.3732590529248
aF94.4958217270195
aF94.61838440111421
aF94.74094707520891
aF94.86350974930362
aF94.98607242339833
aF95.10863509749304
aF95.23119777158774
aF95.35376044568245
aF95.47632311977716
aF95.59888579387187
aF95.72144846796657
aF95.84401114206128
aF95.96657381615599
aF96.0891364902507
aF96.2116991643454
aF96.33426183844011
aF96.45682451253482
aF96.57938718662953
aF96.70194986072423
aF96.82451253481894
aF96.94707520891365
aF97.06963788300835
aF97.19220055710306
aF97.31476323119777
aF97.43732590529248
aF97.55988857938719
aF97.6824512534819
aF97.8050139275766
aF97.9275766016713
aF98.05013927576601
aF98.17270194986072
aF98.29526462395543
aF98.41782729805014
aF98.54038997214485
aF98.66295264623956
aF98.78551532033426
aF98.90807799442896
aF99.03064066852367
aF99.15320334261838
aF99.27576601671309
aF99.3983286908078
aF99.52089136490251
aF99.64345403899722
aF99.76601671309193
aF99.88857938718662
aF100.0
atp26
tp27
Rp28
aa(lp29
S'loss_bbox'
p30
ag0
(g22
g23
(S'L1Loss'
p31
(lp32
F2.54556
aF2.51329
aF4.08554
aF7.30947
aF3.17227
aF4.02298
aF0.8502
aF3.55391
aF0.930148
aF1.41845
aF4.03849
aF3.1178
aF1.39956
aF1.07333
aF1.67979
aF0.31094
aF0.754011
aF1.48949
aF1.52026
aF1.99452
aF1.66226
aF0.657195
aF1.43584
aF1.41329
aF0.999607
aF0.564853
aF1.05496
aF0.576096
aF0.362181
aF0.951512
aF0.8149
aF2.13673
aF0.325103
aF3.18131
aF1.01489
aF0.223128
aF0.831135
aF0.597846
aF0.855533
aF0.427316
aF0.826882
aF0.978731
aF2.18882
aF0.701429
aF0.233076
aF0.385355
aF0.399872
aF0.783116
aF0.575748
aF0.254776
aF0.346047
aF1.05343
aF2.00976
aF0.35386
aF0.764432
aF0.69638
aF0.534579
aF0.644115
aF0.89211
aF0.727486
aF1.06198
aF0.838268
aF0.776219
aF0.507122
aF0.765834
aF0.724671
aF0.491688
aF0.776972
aF0.489418
aF0.715012
aF0.858872
aF1.52428
aF0.785848
aF0.399468
aF0.689826
aF0.500669
aF1.19972
aF0.606312
aF0.560681
aF0.944693
aF0.838764
aF1.1734
aF0.896067
aF0.906971
aF0.762324
aF1.14844
aF0.225838
aF0.367952
aF0.80468
aF0.987218
aF1.57911
aF0.659853
aF0.613457
aF0.278061
aF0.620772
aF0.363083
aF0.650146
aF0.188955
aF0.365549
aF0.48303
aF0.189435
aF1.4948
aF1.00347
aF0.854566
aF0.397685
aF0.684932
aF0.730899
aF0.707466
aF0.580593
aF0.17622
aF0.741357
aF1.16396
aF0.721729
aF0.641289
aF0.489283
aF0.276162
aF1.17037
aF0.504919
aF0.322129
aF0.9591
aF0.710043
aF0.525354
aF0.359054
aF0.51498
aF0.715675
aF0.607591
aF0.841832
aF0.627722
aF0.544556
aF1.17677
aF0.664343
aF0.327385
aF0.88988
aF0.4781
aF0.183258
aF0.685375
aF1.45513
aF0.934165
aF0.692536
aF0.809299
aF0.755477
aF0.414987
aF0.680709
aF1.5056
aF0.745647
aF0.993877
aF0.744338
aF1.13585
aF0.952307
aF1.32107
aF0.565366
aF0.311569
aF0.30767
aF0.885597
aF1.07239
aF0.941617
aF0.512749
aF0.721224
aF0.646674
aF0.301721
aF0.457286
aF0.663722
aF0.394051
aF0.453569
aF0.484049
aF1.36831
aF1.44787
aF1.66351
aF0.774469
aF0.512088
aF1.20632
aF0.476174
aF0.798558
aF0.512044
aF0.527931
aF0.710948
aF0.830009
aF1.10094
aF0.750392
aF0.440634
aF0.923742
aF1.42111
aF0.765426
aF0.970064
aF1.04938
aF1.39568
aF0.740701
aF0.483117
aF0.86425
aF0.662919
aF0.737521
aF0.927879
aF0.496661
aF0.725218
aF0.774312
aF1.15986
aF0.305362
aF1.49438
aF1.3106
aF0.192613
aF1.53469
aF0.840654
aF0.901695
aF0.647829
aF0.993296
aF0.700434
aF0.661416
aF0.652732
aF1.40284
aF0.743001
aF0.598721
aF0.865721
aF1.01272
aF0.84746
aF1.44859
aF0.982226
aF0.824308
aF0.242697
aF0.576585
aF1.28675
aF0.995164
aF0.831212
aF0.501135
aF0.4615
aF0.682247
aF0.917888
aF0.544738
aF1.27023
aF1.48619
aF0.460396
aF0.54919
aF1.03188
aF1.65431
aF1.56688
aF0.674178
aF1.27428
aF0.69561
aF0.290511
aF0.433105
aF0.504774
aF0.501116
aF0.70118
aF0.346324
aF0.320658
aF0.341124
aF0.787416
aF0.47432
aF0.655233
aF0.926608
aF0.735032
aF1.27772
aF1.58234
aF0.738349
aF0.696853
aF0.935948
aF1.08264
aF0.805758
aF1.60744
aF0.30308
aF0.699234
aF0.441347
aF0.634387
aF1.36676
aF0.858611
aF0.325482
aF1.31993
aF0.453055
aF0.827747
aF0.491018
aF0.878233
aF0.82961
aF0.646249
aF0.930336
aF0.643428
aF0.66775
aF0.865836
aF0.717576
aF0.951601
aF1.06468
aF0.719118
aF0.890241
aF1.35196
aF0.69479
aF0.531365
aF0.340248
aF0.406807
aF0.764967
aF1.64691
aF0.298063
aF0.675063
aF0.436643
aF2.14349
aF1.50399
aF0.585692
aF0.62827
aF0.555206
aF0.673742
aF1.01462
aF1.42179
aF0.354404
aF1.0498
aF0.632496
aF0.533594
aF1.22699
aF0.485123
aF0.268226
aF0.937342
aF0.366177
aF1.01629
aF0.611459
aF0.92453
aF0.462808
aF0.469437
aF0.944857
aF0.609928
aF0.512915
aF1.07082
aF1.12173
aF0.654547
aF0.833596
aF0.552242
aF0.351395
aF1.63611
aF1.93741
aF0.790713
aF0.492564
aF0.563602
aF1.06688
aF0.980182
aF0.469998
aF0.40139
aF0.385562
aF1.07404
aF0.477257
aF0.825283
aF0.670148
aF1.12066
aF0.822616
aF1.22084
aF0.451023
aF0.829648
aF1.6085
aF1.18683
aF0.87521
aF0.435738
aF1.31577
aF1.19348
aF0.725891
aF0.181916
aF0.346602
aF0.367274
aF0.377722
aF1.64747
aF0.742395
aF0.692757
aF0.462514
aF0.469508
aF0.296665
aF0.651873
aF0.465975
aF0.514183
aF0.657121
aF0.765512
aF0.578759
aF0.367292
aF1.16393
aF1.35637
aF0.47096
aF0.440736
aF1.63853
aF0.237369
aF3.23446
aF0.661909
aF0.926092
aF0.47519
aF0.419697
aF0.584728
aF0.736318
aF1.19839
aF0.687891
aF0.923692
aF0.755737
aF0.880537
aF1.10342
aF0.298397
aF0.629441
aF0.270339
aF0.252275
aF1.58802
aF0.872199
aF1.28943
aF0.452728
aF0.802848
aF0.571493
aF0.450461
aF1.9774
aF0.403696
aF0.906847
aF0.311806
aF0.651508
aF0.504682
aF0.499212
aF0.422415
aF1.08671
aF1.26525
aF0.374719
aF1.55212
aF0.71982
aF0.236872
aF0.363647
aF0.576587
aF1.41427
aF0.545539
aF0.36694
aF0.605693
aF0.362012
aF0.525998
aF0.874522
aF0.759454
aF1.12472
aF0.578422
aF0.447795
aF1.69489
aF0.650319
aF0.459057
aF0.808397
aF0.591754
aF0.643581
aF0.229652
aF0.756641
aF0.875895
aF0.424026
aF0.315031
aF0.412707
aF0.866847
aF1.08504
aF3.86534
aF0.885704
aF0.871614
aF0.445097
aF0.179426
aF0.749844
aF0.702645
aF0.758547
aF0.982471
aF0.945825
aF0.376242
aF0.881316
aF1.24625
aF0.613942
aF0.551019
aF0.226264
aF0.46152
aF0.594587
aF0.834378
aF0.367284
aF0.216155
aF0.730368
aF0.35697
aF0.19796
aF1.95056
aF0.740016
aF0.716354
aF0.376501
aF0.873778
aF0.661099
aF0.427356
aF0.854252
aF0.194873
aF1.2698
aF0.444913
aF0.594415
aF0.785591
aF0.445004
aF0.257443
aF0.590384
aF0.714849
aF0.251131
aF0.359598
aF0.298307
aF0.46304
aF0.478315
aF0.882015
aF0.543121
aF1.29527
aF0.259317
aF0.636379
aF1.72673
aF0.624232
aF0.678539
aF0.907824
aF0.371567
aF0.378425
aF0.21483
aF0.611798
aF1.43764
aF0.433508
aF0.493231
aF0.684515
aF0.908664
aF0.329938
aF0.637701
aF0.881933
aF0.203093
aF0.851473
aF0.197874
aF1.17445
aF0.813762
aF0.748192
aF0.719144
aF0.199321
aF0.364657
aF1.3298
aF0.746592
aF0.653043
aF0.836942
aF0.547728
aF0.459625
aF0.270401
aF0.33051
aF0.384138
aF0.900807
aF0.338638
aF0.354813
aF0.612336
aF1.28639
aF0.727788
aF0.704723
aF0.345842
aF0.8588
aF0.373947
aF0.616035
aF0.806614
aF1.266
aF0.52876
aF0.479045
aF0.904953
aF0.453459
aF0.418174
aF0.223725
aF0.975461
aF0.636116
aF1.42755
aF0.788622
aF0.30269
aF0.297635
aF0.357823
aF0.810729
aF0.280122
aF0.487031
aF0.243916
aF0.648323
aF0.470991
aF0.58423
aF0.350513
aF0.300139
aF0.369171
aF0.408187
aF0.133337
aF0.534545
aF0.0869249
aF0.877488
aF0.470125
aF0.881267
aF0.574678
aF0.344469
aF0.558601
aF1.06505
aF0.219752
aF0.672491
aF0.697736
aF1.02101
aF0.292345
aF1.17857
aF0.695286
aF0.433108
aF0.196402
aF1.26204
aF0.671215
aF1.41283
aF0.701518
aF0.529662
aF0.388791
aF0.24404
aF0.334057
aF0.507781
aF0.944819
aF0.471123
aF0.392345
aF0.574469
aF0.100823
aF0.904593
aF0.581263
aF0.329258
aF1.06542
aF0.383361
aF0.335361
aF0.398996
aF1.10121
aF0.383474
aF0.637213
aF0.818132
aF0.701655
aF0.317878
aF0.28861
aF1.43172
aF0.647794
aF0.861782
aF0.411456
aF1.06376
aF0.306624
aF0.541757
aF0.604273
aF0.267368
aF0.2864
aF0.717114
aF0.611797
aF1.74625
aF0.575079
aF0.361795
aF0.36676
aF1.39251
aF0.449626
aF1.09208
aF1.29695
aF0.0968705
aF0.97391
aF0.429183
aF0.832462
aF1.07893
aF0.631127
aF0.482675
aF1.52627
aF0.635559
aF0.659401
aF0.661805
aF0.844351
aF0.30124
aF0.548493
aF0.795591
aF0.453752
aF0.573213
aF0.493401
aF0.628875
aF0.355105
aF0.624445
aF0.554399
aF0.247226
aF0.595814
aF0.394958
aF1.09839
aF1.31682
aF0.531843
aF0.255338
aF0.409721
aF0.0897225
aF1.04104
aF1.16135
aF0.333004
aF0.352822
aF0.599009
aF0.303736
aF0.557734
aF0.402212
aF0.433889
aF0.527835
aF0.300235
aF0.8366
aF0.139529
aF0.795518
aF1.54366
aF0.659787
aF0.887437
aF0.384552
aF0.32974
aF1.02951
aF0.495327
aF0.618615
aF0.739645
aF0.234235
aF0.381065
aF1.54051
aF1.35175
aF0.52908
aF0.398944
aF0.334133
aF1.91614
aF0.698268
aF1.37951
aF0.513143
aF0.37174
aF1.1952
aF0.241826
aF0.726572
aF1.38659
aF1.27305
aF0.810615
aF1.5572
aF0.267404
aF0.761837
aF0.645161
aF0.451286
aF0.706791
aF0.568372
aF0.650984
aF1.09447
aF0.592273
aF0.582339
aF0.278659
aF0.365723
aF0.442649
aF1.42918
aF0.248029
aF0.816608
aF0.443142
aF1.35201
aF0.40216
aF0.643772
aF0.275431
aF0.383217
aF0.580855
aF1.24306
aF0.629635
aF0.320789
aF0.29311
aF0.547918
aF0.46146
aF0.550225
aF0.465318
aF0.227949
aF0.444148
aF0.40025
aF0.813072
aF0.15921
aF0.63567
aF0.55427
aF0.612466
aF0.607973
aF0.633827
aF0.317965
aF0.29307
aF0.870243
aF0.533586
aF1.15884
aF0.563448
aF0.427082
aF0.225422
aF1.59189
aF0.532858
aF0.555068
aF0.553749
aF2.0291
aF0.426182
aF0.431907
aF0.654416
aF0.923352
aF0.98306
aF0.25688
aF0.559017
aF0.467906
aF1.37505
aF0.427591
aF1.14815
aF0.279422
aF0.980468
aF1.44668
aF0.428912
aF0.422864
aF0.369553
aF0.604966
aF0.447576
aF0.445885
aF0.617211
aF0.67119
aF0.360916
aF0.445746
aF0.462447
aF0.75444
aF0.718289
aF0.850263
aF0.421168
aF0.383081
aF0.92944
aF0.44275
aF0.330157
aF0.637105
aF0.70406
aF0.600849
aF0.564548
aF0.823933
aF0.533744
aF0.496042
aF0.391705
aF0.487009
aF0.420143
aF3.02764
aF0.456749
aF0.578028
aF0.477977
aF0.876849
aF0.676841
aF0.666609
aF0.617211
aF0.446114
aF0.864516
aF0.301508
aF0.718461
aF1.00689
aF1.16799
aF0.29082
aF0.220508
aF0.297725
aF0.231771
aF0.747561
aF0.607556
aF0.231283
aF0.748569
aF0.449215
aF0.505325
atp33
tp34
Rp35
aa(lp36
S'loss_coverage'
p37
ag0
(g22
g23
(S'EuclideanLoss'
p38
(lp39
F151.248
aF94.3619
aF74.2478
aF33.3007
aF22.7977
aF20.0254
aF16.6176
aF27.1138
aF18.3534
aF27.5529
aF45.1523
aF12.5023
aF32.3541
aF32.7023
aF24.7524
aF7.95209
aF24.5538
aF17.4012
aF20.101
aF42.6056
aF20.4018
aF16.9025
aF17.8563
aF34.2506
aF29.1514
aF24.6525
aF28.1009
aF26.3023
aF17.8019
aF23.0009
aF34.7007
aF24.0004
aF10.2504
aF42.2507
aF20.5003
aF9.00056
aF21.4503
aF17.8506
aF38.3001
aF20.0002
aF24.1509
aF31.9001
aF12.5502
aF28.9522
aF5.25079
aF13.2004
aF12.6506
aF29.2002
aF22.8003
aF14.8505
aF17.4001
aF13.6504
aF21.5503
aF23.4004
aF52.8002
aF19.2001
aF33.0001
aF24.9502
aF14.1502
aF25.8502
aF26.6001
aF22.9501
aF24.1002
aF20.6002
aF23.6502
aF26.3504
aF16.5
aF21.3512
aF14.9501
aF28.8502
aF19.4501
aF15.1502
aF30.3501
aF17.8501
aF22.8001
aF27.5502
aF12.3501
aF13.7503
aF29.1001
aF26.4001
aF28.1537
aF11.1
aF20.4501
aF26.5001
aF23.7501
aF23.2001
aF17.2001
aF16.0502
aF34.7501
aF39.4501
aF32.4001
aF30.5502
aF25.3501
aF12.2501
aF24.1001
aF19.4501
aF35.6001
aF9.64993
aF13.6003
aF15.4643
aF6.22222
aF37.079
aF7.78852
aF30.6207
aF14.0388
aF24.3455
aF19.0924
aF25.6254
aF19.333
aF2.51132
aF14.4735
aF20.4184
aF23.3539
aF15.3668
aF6.86475
aF7.16632
aF25.3324
aF7.97742
aF22.8214
aF20.0352
aF14.7378
aF17.8479
aF10.6156
aF10.3159
aF13.7351
aF13.0002
aF15.0504
aF9.18305
aF12.1113
aF12.7238
aF15.5098
aF5.69988
aF17.2373
aF7.85934
aF3.86278
aF11.4334
aF7.39006
aF6.7561
aF11.4552
aF10.6977
aF10.0017
aF6.73699
aF4.94529
aF15.0008
aF14.3068
aF15.0686
aF12.4682
aF12.2596
aF7.04467
aF13.3185
aF13.6525
aF7.02616
aF3.63007
aF17.3306
aF17.6947
aF4.21783
aF7.92212
aF15.3732
aF8.36894
aF1.65576
aF7.95342
aF9.37531
aF4.90222
aF5.60409
aF4.28047
aF10.6882
aF16.2521
aF5.50026
aF12.6965
aF8.57107
aF10.4182
aF7.67152
aF15.0626
aF8.7652
aF4.19011
aF9.09826
aF6.10193
aF6.52799
aF8.82562
aF6.38607
aF10.4075
aF15.5575
aF2.97843
aF10.4632
aF12.1445
aF6.76753
aF6.7926
aF7.98468
aF15.5647
aF9.48807
aF8.86876
aF7.5131
aF9.29089
aF8.10747
aF8.08547
aF9.49662
aF4.81155
aF13.8496
aF7.53588
aF3.34149
aF13.1472
aF15.6299
aF5.60989
aF11.5594
aF11.3674
aF8.33458
aF9.26587
aF13.981
aF10.9576
aF7.57102
aF3.43368
aF10.3979
aF7.3022
aF6.446
aF8.58265
aF10.1903
aF5.06781
aF3.90658
aF5.91319
aF10.6539
aF3.05512
aF13.6585
aF6.89975
aF7.81243
aF10.2746
aF7.70751
aF6.77871
aF6.22655
aF13.1931
aF4.0495
aF6.88095
aF12.455
aF5.79519
aF9.92738
aF8.35262
aF8.59194
aF7.52159
aF3.03349
aF8.61312
aF10.4217
aF5.18694
aF5.87754
aF5.53446
aF3.47248
aF5.05641
aF8.71067
aF8.53127
aF2.50898
aF10.1291
aF7.74958
aF5.90894
aF5.91538
aF8.41835
aF15.0355
aF14.2346
aF6.51322
aF6.76399
aF6.16979
aF5.60491
aF6.69985
aF3.26404
aF5.65895
aF9.83208
aF5.65179
aF4.44774
aF14.378
aF9.60672
aF8.81122
aF7.53166
aF7.82099
aF5.26871
aF7.28985
aF8.66756
aF6.50943
aF5.47273
aF9.61139
aF9.1628
aF6.5926
aF7.47054
aF4.6324
aF9.66034
aF6.54739
aF4.07019
aF4.12768
aF4.38077
aF3.68959
aF10.1949
aF8.83445
aF6.81761
aF9.28414
aF3.74823
aF18.0233
aF7.08103
aF6.8386
aF3.62865
aF7.37252
aF8.05223
aF6.28993
aF7.83151
aF4.60884
aF6.71263
aF8.41882
aF9.48352
aF11.2434
aF8.40876
aF4.92129
aF11.9143
aF5.95861
aF11.3526
aF4.71269
aF8.60874
aF3.36018
aF4.45216
aF9.36449
aF4.02329
aF11.3239
aF5.13933
aF9.78549
aF6.54788
aF10.4585
aF4.21626
aF7.1616
aF6.36498
aF4.67422
aF5.2486
aF4.1551
aF3.44987
aF14.6808
aF5.6866
aF6.93571
aF5.9383
aF8.36071
aF5.45627
aF2.66975
aF7.11509
aF2.33093
aF8.2595
aF7.53074
aF8.72001
aF13.0978
aF8.20135
aF8.21381
aF10.9157
aF7.61652
aF4.44308
aF6.71191
aF6.02861
aF4.47087
aF4.55965
aF4.59007
aF5.22157
aF4.76136
aF7.26449
aF7.59754
aF9.5013
aF2.50292
aF6.92772
aF2.99936
aF4.61648
aF2.71259
aF3.79799
aF8.55852
aF8.73731
aF4.65647
aF5.18763
aF6.03015
aF6.6628
aF8.71902
aF4.01413
aF14.8926
aF3.93985
aF9.01567
aF10.3048
aF11.547
aF5.08232
aF4.89103
aF3.04598
aF9.75742
aF9.77553
aF3.20731
aF12.8635
aF5.45194
aF7.69969
aF6.46358
aF4.44658
aF4.88588
aF3.55584
aF4.0395
aF4.58623
aF3.15931
aF6.50807
aF2.3417
aF8.24224
aF3.41054
aF8.24284
aF9.27642
aF3.84024
aF5.71844
aF3.59604
aF7.57956
aF2.59611
aF4.13658
aF6.55007
aF7.01357
aF12.731
aF5.42917
aF7.8054
aF4.97822
aF4.07275
aF3.84376
aF4.29017
aF6.94123
aF3.10402
aF6.27117
aF6.48209
aF4.25094
aF3.92035
aF8.04817
aF6.90489
aF4.66879
aF6.82286
aF8.53242
aF8.32158
aF4.37896
aF3.14162
aF9.63959
aF5.5025
aF7.45715
aF2.1913
aF4.4053
aF6.27463
aF5.73748
aF3.86448
aF3.62431
aF5.83667
aF4.27013
aF9.02885
aF7.23993
aF8.10711
aF5.08794
aF3.28331
aF4.1444
aF7.32132
aF9.34895
aF5.57443
aF10.1768
aF6.48984
aF8.02845
aF7.76694
aF5.96349
aF6.38133
aF3.03275
aF3.98156
aF5.84979
aF3.47604
aF6.65675
aF1.58561
aF6.06962
aF3.30049
aF1.65576
aF8.49873
aF5.76531
aF9.43496
aF4.15047
aF5.16388
aF8.33945
aF3.2495
aF5.74839
aF1.11762
aF12.1638
aF4.11123
aF7.27086
aF4.17745
aF8.03837
aF2.3103
aF5.44154
aF6.58402
aF4.53682
aF3.4887
aF4.2391
aF7.80196
aF2.83177
aF8.59748
aF6.23345
aF4.40639
aF8.97502
aF5.0939
aF8.07875
aF4.5691
aF4.92384
aF7.27165
aF7.14641
aF3.58766
aF2.40183
aF5.12786
aF2.82046
aF6.10424
aF3.62232
aF7.18512
aF4.78552
aF3.14925
aF4.4322
aF6.50482
aF1.75461
aF8.74864
aF2.84603
aF12.1627
aF4.52104
aF10.3303
aF7.42051
aF3.574
aF5.07927
aF8.41056
aF9.00834
aF5.4876
aF4.30018
aF5.12059
aF4.62778
aF3.67277
aF3.53515
aF8.05342
aF8.19641
aF3.10603
aF2.91955
aF8.07601
aF11.5414
aF4.78889
aF6.49398
aF4.3518
aF5.95686
aF3.29106
aF6.41061
aF5.17604
aF6.58103
aF9.03663
aF4.12377
aF8.82562
aF5.48627
aF6.20313
aF2.52773
aF9.10894
aF5.12466
aF3.29223
aF6.44189
aF3.71296
aF8.66395
aF4.81102
aF8.89346
aF4.09642
aF6.62092
aF4.78739
aF4.59042
aF6.641
aF3.98724
aF2.96774
aF4.09651
aF5.09876
aF3.13042
aF1.54231
aF4.2243
aF0.24548
aF4.44093
aF3.19871
aF13.9159
aF4.52773
aF3.21729
aF7.49486
aF5.88295
aF1.55632
aF8.43468
aF7.06744
aF12.3218
aF1.68267
aF6.79257
aF7.26932
aF4.33133
aF2.43634
aF9.33501
aF9.14067
aF2.32372
aF4.14299
aF7.15695
aF5.31319
aF3.646
aF5.21573
aF6.00486
aF8.46963
aF2.45371
aF2.89479
aF6.99486
aF1.50427
aF2.80401
aF5.82013
aF4.91512
aF4.58476
aF3.20815
aF3.08247
aF8.3372
aF6.29594
aF5.45174
aF4.90833
aF7.59453
aF8.13373
aF4.13594
aF2.91089
aF8.0827
aF2.4208
aF2.91584
aF5.40853
aF5.95366
aF7.90051
aF6.59397
aF9.61165
aF4.22698
aF5.09515
aF5.26212
aF4.78053
aF6.76022
aF4.30529
aF4.30889
aF2.29103
aF7.59189
aF3.54707
aF3.67759
aF10.7005
aF0.422807
aF5.28707
aF6.76846
aF13.2567
aF3.35228
aF5.08281
aF4.98297
aF14.2622
aF4.05458
aF9.93738
aF7.24926
aF4.82449
aF2.51269
aF2.28604
aF6.97969
aF5.4519
aF7.50009
aF3.71607
aF6.06355
aF4.35078
aF7.16839
aF7.12361
aF1.76015
aF8.26601
aF5.99557
aF9.65241
aF4.98474
aF2.38793
aF2.40363
aF4.93702
aF1.51662
aF7.25346
aF6.19468
aF4.0622
aF5.66722
aF5.80827
aF2.47507
aF5.34693
aF5.88051
aF7.0754
aF5.50319
aF5.33275
aF8.94483
aF1.03085
aF8.48014
aF8.63776
aF6.47782
aF7.73048
aF6.39118
aF3.25945
aF4.56336
aF4.46012
aF5.53234
aF10.0326
aF4.08297
aF4.84407
aF5.92242
aF6.8517
aF6.66073
aF2.74199
aF2.85148
aF12.0761
aF3.78105
aF4.20809
aF4.74431
aF6.89735
aF6.12116
aF3.687
aF5.83494
aF3.03123
aF5.03443
aF6.26522
aF11.4781
aF4.81705
aF6.69405
aF6.32011
aF5.74965
aF5.24275
aF2.94232
aF4.26903
aF4.44732
aF7.6678
aF8.85393
aF2.2032
aF4.79191
aF4.01294
aF7.99646
aF1.21421
aF5.43327
aF3.14149
aF12.4092
aF4.84444
aF4.02512
aF2.73769
aF3.07773
aF5.78191
aF7.56921
aF6.3516
aF2.98108
aF6.03267
aF5.6836
aF9.00832
aF7.62607
aF5.24748
aF2.43153
aF5.20889
aF4.63942
aF7.55694
aF0.939366
aF3.12713
aF2.35681
aF5.60907
aF4.77127
aF3.18467
aF3.51784
aF2.70702
aF7.63266
aF5.57247
aF6.80227
aF4.32169
aF4.74527
aF3.48562
aF3.96497
aF5.5663
aF6.62535
aF2.27314
aF11.2825
aF4.74204
aF3.63893
aF3.92861
aF3.5996
aF5.70626
aF3.51495
aF6.54832
aF2.02642
aF4.13917
aF4.5729
aF5.6488
aF2.9906
aF8.83859
aF6.55639
aF4.6546
aF7.26962
aF3.80182
aF3.36435
aF3.3288
aF2.62571
aF9.05894
aF5.0098
aF3.45121
aF4.26253
aF6.23763
aF5.2891
aF5.06079
aF3.62801
aF6.34481
aF4.62979
aF6.29025
aF2.36627
aF4.4183
aF10.6517
aF8.52407
aF6.36296
aF5.62215
aF6.52565
aF6.08624
aF3.49046
aF3.87208
aF4.90216
aF3.97331
aF8.11864
aF6.78837
aF6.80834
aF5.51138
aF2.9472
aF4.25235
aF9.01363
aF4.82847
aF4.30402
aF8.7911
aF2.74941
aF6.94769
aF6.26649
aF6.68822
aF3.61758
aF2.53995
aF4.51027
aF1.86867
aF2.15333
aF6.21723
aF3.60311
aF4.88829
aF4.58687
aF6.76102
atp40
tp41
Rp42
aa(lp43
S'learning_rate'
p44
ag0
(g22
g23
(S'LearningRate'
p45
(lp46
F2.5e-05
aNaNaF2.49069e-05
aF2.48141e-05
aF2.47217e-05
aF2.46296e-05
aF2.45378e-05
aF2.44464e-05
aF2.43553e-05
aF2.42646e-05
aF2.41742e-05
aF2.40842e-05
aF2.39944e-05
aF2.39051e-05
aF2.3816e-05
aF2.37273e-05
aF2.36389e-05
aF2.35508e-05
aF2.34631e-05
aF2.33757e-05
aF2.32886e-05
aF2.32019e-05
aF2.31154e-05
aF2.30293e-05
aF2.29436e-05
aF2.28581e-05
aF2.27729e-05
aF2.26881e-05
aF2.26036e-05
aF2.25194e-05
aF2.24355e-05
aF2.23519e-05
aF2.22687e-05
aF2.21857e-05
aF2.21031e-05
aF2.20207e-05
aF2.19387e-05
aF2.1857e-05
aF2.17755e-05
aF2.16944e-05
aF2.16136e-05
aF2.15331e-05
aF2.14529e-05
aF2.1373e-05
aF2.12933e-05
aF2.1214e-05
aF2.1135e-05
aF2.10563e-05
aF2.09778e-05
aF2.08997e-05
aF2.08218e-05
aF2.07443e-05
aF2.0667e-05
aF2.059e-05
aF2.05133e-05
aF2.04369e-05
aF2.03607e-05
aF2.02849e-05
aF2.02093e-05
aF2.01341e-05
aF2.00591e-05
aF1.99843e-05
aF1.99099e-05
aF1.98357e-05
aF1.97618e-05
aF1.96882e-05
aF1.96149e-05
aF1.95418e-05
aF1.9469e-05
aF1.93965e-05
aF1.93242e-05
aF1.92522e-05
aF1.91805e-05
aF1.91091e-05
aF1.90379e-05
aF1.8967e-05
aF1.88963e-05
aF1.88259e-05
aF1.87558e-05
aF1.86859e-05
aF1.86163e-05
aF1.8547e-05
aF1.84779e-05
aF1.8409e-05
aF1.83405e-05
aF1.82721e-05
aF1.82041e-05
aF1.81363e-05
aF1.80687e-05
aF1.80014e-05
aF1.79343e-05
aF1.78675e-05
aF1.7801e-05
aF1.77347e-05
aF1.76686e-05
aF1.76028e-05
aF1.75372e-05
aF1.74719e-05
aF1.74068e-05
aF1.73419e-05
aF1.72773e-05
aF1.7213e-05
aF1.71489e-05
aF1.7085e-05
aF1.70213e-05
aF1.69579e-05
aF1.68947e-05
aF1.68318e-05
aF1.67691e-05
aF1.67066e-05
aF1.66444e-05
aF1.65824e-05
aF1.65206e-05
aF1.64591e-05
aF1.63978e-05
aF1.63367e-05
aF1.62758e-05
aF1.62152e-05
aF1.61548e-05
aF1.60946e-05
aF1.60347e-05
aF1.59749e-05
aF1.59154e-05
aF1.58561e-05
aF1.57971e-05
aF1.57382e-05
aF1.56796e-05
aF1.56212e-05
aF1.5563e-05
aF1.5505e-05
aF1.54473e-05
aF1.53897e-05
aF1.53324e-05
aF1.52753e-05
aF1.52184e-05
aF1.51617e-05
aF1.51052e-05
aF1.50489e-05
aF1.49929e-05
aF1.4937e-05
aF1.48814e-05
aF1.48259e-05
aF1.47707e-05
aF1.47157e-05
aF1.46609e-05
aF1.46063e-05
aF1.45518e-05
aF1.44976e-05
aF1.44436e-05
aF1.43898e-05
aF1.43362e-05
aF1.42828e-05
aF1.42296e-05
aF1.41766e-05
aF1.41238e-05
aF1.40712e-05
aF1.40188e-05
aF1.39665e-05
aF1.39145e-05
aF1.38627e-05
aF1.3811e-05
aF1.37596e-05
aF1.37083e-05
aF1.36573e-05
aF1.36064e-05
aF1.35557e-05
aF1.35052e-05
aF1.34549e-05
aF1.34048e-05
aF1.33548e-05
aF1.33051e-05
aF1.32555e-05
aF1.32061e-05
aF1.3157e-05
aF1.31079e-05
aF1.30591e-05
aF1.30105e-05
aF1.2962e-05
aF1.29137e-05
aF1.28656e-05
aF1.28177e-05
aF1.27699e-05
aF1.27224e-05
aF1.2675e-05
aF1.26278e-05
aF1.25807e-05
aF1.25338e-05
aF1.24872e-05
aF1.24406e-05
aF1.23943e-05
aF1.23481e-05
aF1.23021e-05
aF1.22563e-05
aF1.22106e-05
aF1.21652e-05
aF1.21198e-05
aF1.20747e-05
aF1.20297e-05
aF1.19849e-05
aF1.19403e-05
aF1.18958e-05
aF1.18515e-05
aF1.18073e-05
aF1.17633e-05
aF1.17195e-05
aF1.16758e-05
aF1.16324e-05
aF1.1589e-05
aF1.15458e-05
aF1.15028e-05
aF1.146e-05
aF1.14173e-05
aF1.13748e-05
aF1.13324e-05
aF1.12902e-05
aF1.12481e-05
aF1.12062e-05
aF1.11645e-05
aF1.11229e-05
aF1.10815e-05
aF1.10402e-05
aF1.0999e-05
aF1.09581e-05
aF1.09173e-05
aF1.08766e-05
aF1.08361e-05
aF1.07957e-05
aF1.07555e-05
aF1.07154e-05
aF1.06755e-05
aF1.06357e-05
aF1.05961e-05
aF1.05566e-05
aF1.05173e-05
aF1.04781e-05
aF1.04391e-05
aF1.04002e-05
aF1.03615e-05
aF1.03229e-05
aF1.02844e-05
aF1.02461e-05
aF1.02079e-05
aF1.01699e-05
aF1.0132e-05
aF1.00943e-05
aF1.00567e-05
aF1.00192e-05
aF9.9819e-06
aF9.94471e-06
aF9.90767e-06
aF9.87076e-06
aF9.83399e-06
aF9.79736e-06
aF9.76086e-06
aF9.7245e-06
aF9.68827e-06
aF9.65218e-06
aF9.61623e-06
aF9.58041e-06
aF9.54472e-06
aF9.50916e-06
aF9.47374e-06
aF9.43845e-06
aF9.40329e-06
aF9.36826e-06
aF9.33336e-06
aF9.29859e-06
aF9.26395e-06
aF9.22944e-06
aF9.19506e-06
aF9.16081e-06
aF9.12668e-06
aF9.09268e-06
aF9.05881e-06
aF9.02507e-06
aF8.99145e-06
aF8.95795e-06
aF8.92458e-06
aF8.89134e-06
aF8.85821e-06
aF8.82522e-06
aF8.79234e-06
aF8.75959e-06
aF8.72696e-06
aF8.69445e-06
aF8.66206e-06
aF8.62979e-06
aF8.59764e-06
aF8.56562e-06
aF8.53371e-06
aF8.50192e-06
aF8.47025e-06
aF8.43869e-06
aF8.40726e-06
aF8.37594e-06
aF8.34474e-06
aF8.31365e-06
aF8.28268e-06
aF8.25183e-06
aF8.22109e-06
aF8.19046e-06
aF8.15995e-06
aF8.12956e-06
aF8.09927e-06
aF8.0691e-06
aF8.03904e-06
aF8.0091e-06
aF7.97926e-06
aF7.94954e-06
aF7.91992e-06
aF7.89042e-06
aF7.86103e-06
aF7.83174e-06
aF7.80257e-06
aF7.7735e-06
aF7.74455e-06
aF7.7157e-06
aF7.68695e-06
aF7.65832e-06
aF7.62979e-06
aF7.60137e-06
aF7.57305e-06
aF7.54484e-06
aF7.51673e-06
aF7.48873e-06
aF7.46084e-06
aF7.43304e-06
aF7.40535e-06
aF7.37777e-06
aF7.35028e-06
aF7.3229e-06
aF7.29562e-06
aF7.26845e-06
aF7.24137e-06
aF7.2144e-06
aF7.18752e-06
aF7.16075e-06
aF7.13407e-06
aF7.1075e-06
aF7.08102e-06
aF7.05464e-06
aF7.02836e-06
aF7.00218e-06
aF6.97609e-06
aF6.95011e-06
aF6.92422e-06
aF6.89842e-06
aF6.87273e-06
aF6.84712e-06
aF6.82162e-06
aF6.79621e-06
aF6.77089e-06
aF6.74567e-06
aF6.72054e-06
aF6.6955e-06
aF6.67056e-06
aF6.64571e-06
aF6.62095e-06
aF6.59629e-06
aF6.57172e-06
aF6.54724e-06
aF6.52285e-06
aF6.49855e-06
aF6.47434e-06
aF6.45022e-06
aF6.42619e-06
aF6.40226e-06
aF6.37841e-06
aF6.35465e-06
aF6.33097e-06
aF6.30739e-06
aF6.28389e-06
aF6.26048e-06
aF6.23716e-06
aF6.21393e-06
aF6.19078e-06
aF6.16772e-06
aF6.14474e-06
aF6.12185e-06
aF6.09905e-06
aF6.07633e-06
aF6.05369e-06
aF6.03114e-06
aF6.00868e-06
aF5.98629e-06
aF5.96399e-06
aF5.94178e-06
aF5.91964e-06
aF5.89759e-06
aF5.87562e-06
aF5.85373e-06
aF5.83193e-06
aF5.8102e-06
aF5.78856e-06
aF5.76699e-06
aF5.74551e-06
aF5.72411e-06
aF5.70278e-06
aF5.68154e-06
aF5.66038e-06
aF5.63929e-06
aF5.61828e-06
aF5.59735e-06
aF5.5765e-06
aF5.55573e-06
aF5.53503e-06
aF5.51441e-06
aF5.49387e-06
aF5.47341e-06
aF5.45302e-06
aF5.4327e-06
aF5.41247e-06
aF5.3923e-06
aF5.37222e-06
aF5.3522e-06
aF5.33227e-06
aF5.3124e-06
aF5.29261e-06
aF5.2729e-06
aF5.25325e-06
aF5.23369e-06
aF5.21419e-06
aF5.19477e-06
aF5.17541e-06
aF5.15613e-06
aF5.13693e-06
aF5.11779e-06
aF5.09873e-06
aF5.07973e-06
aF5.06081e-06
aF5.04196e-06
aF5.02318e-06
aF5.00446e-06
aF4.98582e-06
aF4.96725e-06
aF4.94874e-06
aF4.93031e-06
aF4.91194e-06
aF4.89365e-06
aF4.87542e-06
aF4.85725e-06
aF4.83916e-06
aF4.82113e-06
aF4.80317e-06
aF4.78528e-06
aF4.76745e-06
aF4.7497e-06
aF4.732e-06
aF4.71437e-06
aF4.69681e-06
aF4.67932e-06
aF4.66188e-06
aF4.64452e-06
aF4.62722e-06
aF4.60998e-06
aF4.59281e-06
aF4.5757e-06
aF4.55865e-06
aF4.54167e-06
aF4.52475e-06
aF4.5079e-06
aF4.4911e-06
aF4.47437e-06
aF4.45771e-06
aF4.4411e-06
aF4.42456e-06
aF4.40807e-06
aF4.39165e-06
aF4.37529e-06
aF4.359e-06
aF4.34276e-06
aF4.32658e-06
aF4.31046e-06
aF4.29441e-06
aF4.27841e-06
aF4.26247e-06
aF4.24659e-06
aF4.23077e-06
aF4.21501e-06
aF4.19931e-06
aF4.18367e-06
aF4.16808e-06
aF4.15256e-06
aF4.13709e-06
aF4.12168e-06
aF4.10632e-06
aF4.09102e-06
aF4.07579e-06
aF4.0606e-06
aF4.04548e-06
aF4.03041e-06
aF4.01539e-06
aF4.00043e-06
aF3.98553e-06
aF3.97068e-06
aF3.95589e-06
aF3.94116e-06
aF3.92648e-06
aF3.91185e-06
aF3.89728e-06
aF3.88276e-06
aF3.86829e-06
aF3.85388e-06
aF3.83953e-06
aF3.82523e-06
aF3.81098e-06
aF3.79678e-06
aF3.78264e-06
aF3.76854e-06
aF3.75451e-06
aF3.74052e-06
aF3.72659e-06
aF3.7127e-06
aF3.69887e-06
aF3.68509e-06
aF3.67137e-06
aF3.65769e-06
aF3.64406e-06
aF3.63049e-06
aF3.61697e-06
aF3.60349e-06
aF3.59007e-06
aF3.57669e-06
aF3.56337e-06
aF3.5501e-06
aF3.53687e-06
aF3.5237e-06
aF3.51057e-06
aF3.49749e-06
aF3.48446e-06
aF3.47148e-06
aF3.45855e-06
aF3.44567e-06
aF3.43283e-06
aF3.42004e-06
aF3.4073e-06
aF3.39461e-06
aF3.38197e-06
aF3.36937e-06
aF3.35682e-06
aF3.34431e-06
aF3.33185e-06
aF3.31944e-06
aF3.30708e-06
aF3.29476e-06
aF3.28248e-06
aF3.27026e-06
aF3.25807e-06
aF3.24594e-06
aF3.23384e-06
aF3.2218e-06
aF3.2098e-06
aF3.19784e-06
aF3.18593e-06
aF3.17406e-06
aF3.16223e-06
aF3.15045e-06
aF3.13872e-06
aF3.12703e-06
aF3.11538e-06
aF3.10377e-06
aF3.09221e-06
aF3.08069e-06
aF3.06922e-06
aF3.05778e-06
aF3.04639e-06
aF3.03504e-06
aF3.02374e-06
aF3.01247e-06
aF3.00125e-06
aF2.99007e-06
aF2.97893e-06
aF2.96784e-06
aF2.95678e-06
aF2.94577e-06
aF2.93479e-06
aF2.92386e-06
aF2.91297e-06
aF2.90212e-06
aF2.89131e-06
aF2.88053e-06
aF2.8698e-06
aF2.85911e-06
aF2.84846e-06
aF2.83785e-06
aF2.82728e-06
aF2.81675e-06
aF2.80626e-06
aF2.7958e-06
aF2.78539e-06
aF2.77501e-06
aF2.76467e-06
aF2.75437e-06
aF2.74411e-06
aF2.73389e-06
aF2.72371e-06
aF2.71356e-06
aF2.70345e-06
aF2.69338e-06
aF2.68335e-06
aF2.67335e-06
aF2.66339e-06
aF2.65347e-06
aF2.64359e-06
aF2.63374e-06
aF2.62393e-06
aF2.61415e-06
aF2.60442e-06
aF2.59471e-06
aF2.58505e-06
aF2.57542e-06
aF2.56582e-06
aF2.55627e-06
aF2.54674e-06
aF2.53726e-06
aF2.52781e-06
aF2.51839e-06
aF2.50901e-06
aF2.49966e-06
aF2.49035e-06
aF2.48107e-06
aF2.47183e-06
aF2.46262e-06
aF2.45345e-06
aF2.44431e-06
aF2.4352e-06
aF2.42613e-06
aF2.41709e-06
aF2.40809e-06
aF2.39912e-06
aF2.39018e-06
aF2.38128e-06
aF2.37241e-06
aF2.36357e-06
aF2.35477e-06
aF2.34599e-06
aF2.33725e-06
aF2.32855e-06
aF2.31987e-06
aF2.31123e-06
aF2.30262e-06
aF2.29404e-06
aF2.2855e-06
aF2.27698e-06
aF2.2685e-06
aF2.26005e-06
aF2.25163e-06
aF2.24325e-06
aF2.23489e-06
aF2.22656e-06
aF2.21827e-06
aF2.21001e-06
aF2.20177e-06
aF2.19357e-06
aF2.1854e-06
aF2.17726e-06
aF2.16915e-06
aF2.16107e-06
aF2.15302e-06
aF2.145e-06
aF2.13701e-06
aF2.12905e-06
aF2.12111e-06
aF2.11321e-06
aF2.10534e-06
aF2.0975e-06
aF2.08968e-06
aF2.0819e-06
aF2.07414e-06
aF2.06642e-06
aF2.05872e-06
aF2.05105e-06
aF2.04341e-06
aF2.0358e-06
aF2.02822e-06
aF2.02066e-06
aF2.01313e-06
aF2.00563e-06
aF1.99816e-06
aF1.99072e-06
aF1.9833e-06
aF1.97591e-06
aF1.96855e-06
aF1.96122e-06
aF1.95391e-06
aF1.94664e-06
aF1.93938e-06
aF1.93216e-06
aF1.92496e-06
aF1.91779e-06
aF1.91065e-06
aF1.90353e-06
aF1.89644e-06
aF1.88937e-06
aF1.88234e-06
aF1.87532e-06
aF1.86834e-06
aF1.86138e-06
aF1.85444e-06
aF1.84754e-06
aF1.84065e-06
aF1.8338e-06
aF1.82697e-06
aF1.82016e-06
aF1.81338e-06
aF1.80662e-06
aF1.79989e-06
aF1.79319e-06
aF1.78651e-06
aF1.77985e-06
aF1.77322e-06
aF1.76662e-06
aF1.76004e-06
aF1.75348e-06
aF1.74695e-06
aF1.74044e-06
aF1.73396e-06
aF1.7275e-06
aF1.72106e-06
aF1.71465e-06
aF1.70827e-06
aF1.7019e-06
aF1.69556e-06
aF1.68925e-06
aF1.68295e-06
aF1.67668e-06
aF1.67044e-06
aF1.66422e-06
aF1.65802e-06
aF1.65184e-06
aF1.64569e-06
aF1.63956e-06
aF1.63345e-06
aF1.62736e-06
aF1.6213e-06
aF1.61526e-06
aF1.60924e-06
aF1.60325e-06
aF1.59728e-06
aF1.59133e-06
aF1.5854e-06
aF1.57949e-06
aF1.57361e-06
aF1.56775e-06
aF1.56191e-06
aF1.55609e-06
aF1.55029e-06
aF1.54452e-06
aF1.53876e-06
aF1.53303e-06
aF1.52732e-06
aF1.52163e-06
aF1.51596e-06
aF1.51032e-06
aF1.50469e-06
aF1.49908e-06
aF1.4935e-06
aF1.48794e-06
aF1.48239e-06
aF1.47687e-06
aF1.47137e-06
aF1.46589e-06
aF1.46043e-06
aF1.45499e-06
aF1.44957e-06
aF1.44417e-06
aF1.43879e-06
aF1.43343e-06
aF1.42809e-06
aF1.42277e-06
aF1.41747e-06
aF1.41219e-06
aF1.40693e-06
aF1.40169e-06
aF1.39646e-06
aF1.39126e-06
aF1.38608e-06
aF1.38092e-06
aF1.37577e-06
aF1.37065e-06
aF1.36554e-06
aF1.36045e-06
aF1.35539e-06
aF1.35034e-06
aF1.34531e-06
aF1.3403e-06
aF1.3353e-06
aF1.33033e-06
aF1.32537e-06
aF1.32044e-06
aF1.31552e-06
aF1.31062e-06
aF1.30573e-06
aF1.30087e-06
aF1.29602e-06
aF1.2912e-06
aF1.28639e-06
aF1.28159e-06
aF1.27682e-06
aF1.27206e-06
aF1.26733e-06
aF1.2626e-06
aF1.2579e-06
aF1.25321e-06
aF1.24855e-06
aF1.2439e-06
aF1.23926e-06
aF1.23465e-06
aF1.23005e-06
aF1.22546e-06
aF1.2209e-06
aF1.21635e-06
aF1.21182e-06
aF1.20731e-06
aF1.20281e-06
aF1.19833e-06
aF1.19386e-06
atp47
tp48
Rp49
aatp50
Rp51
sS'val_outputs'
p52
g18
((lp53
(lp54
g21
ag0
(g22
g23
(g24
(lp55
I0
aF1.0
aF2.0
aF3.0
aF4.0
aF5.0
aF6.0
aF7.0
aF8.0
aF9.0
aF10.0
aF11.0
aF12.0
aF13.0
aF14.0
aF15.0
aF16.0
aF17.0
aF18.0
aF19.0
aF20.0
aF21.0
aF22.0
aF23.0
aF24.0
aF25.0
aF26.0
aF27.0
aF28.0
aF29.0
aF30.0
aF31.0
aF32.0
aF33.0
aF34.0
aF35.0
aF36.0
aF37.0
aF38.0
aF39.0
aF40.0
aF41.0
aF42.0
aF43.0
aF44.0
aF45.0
aF46.0
aF47.0
aF48.0
aF49.0
aF50.0
aF51.0
aF52.0
aF53.0
aF54.0
aF55.0
aF56.0
aF57.0
aF58.0
aF59.0
aF60.0
aF61.0
aF62.0
aF63.0
aF64.0
aF65.0
aF66.0
aF67.0
aF68.0
aF69.0
aF70.0
aF71.0
aF72.0
aF73.0
aF74.0
aF75.0
aF76.0
aF77.0
aF78.0
aF79.0
aF80.0
aF81.0
aF82.0
aF83.0
aF84.0
aF85.0
aF86.0
aF87.0
aF88.0
aF89.0
aF90.0
aF91.0
aF92.0
aF93.0
aF94.0
aF95.0
aF96.0
aF97.0
aF98.0
aF99.0
aF100.0
atp56
tp57
Rp58
aa(lp59
S'loss_bbox'
p60
ag0
(g22
g23
(S'L1Loss'
p61
(lp62
F5.01033
aF1.14998
aF0.985657
aF0.771235
aF0.67437
aF0.6284
aF0.607489
aF0.670435
aF0.573583
aF0.583041
aF0.639338
aF0.545705
aF0.662534
aF0.615023
aF0.605562
aF0.610949
aF0.587781
aF0.589009
aF0.637678
aF0.658377
aF0.649406
aF0.63954
aF0.627743
aF0.608186
aF0.59487
aF0.586396
aF0.574477
aF0.572396
aF0.56478
aF0.562135
aF0.551415
aF0.548632
aF0.555054
aF0.542005
aF0.540324
aF0.532782
aF0.534397
aF0.532496
aF0.530591
aF0.526725
aF0.527082
aF0.526788
aF0.532212
aF0.521147
aF0.520563
aF0.524325
aF0.519755
aF0.516277
aF0.515819
aF0.51424
aF0.515704
aF0.515061
aF0.510288
aF0.517079
aF0.510638
aF0.507762
aF0.512497
aF0.510356
aF0.512079
aF0.506968
aF0.507496
aF0.507224
aF0.504239
aF0.504494
aF0.502023
aF0.502735
aF0.503445
aF0.497649
aF0.502001
aF0.500367
aF0.499635
aF0.503192
aF0.497471
aF0.498687
aF0.500396
aF0.499453
aF0.49871
aF0.501678
aF0.497636
aF0.496854
aF0.496165
aF0.496194
aF0.500025
aF0.496623
aF0.495961
aF0.494287
aF0.494559
aF0.495098
aF0.491614
aF0.494654
aF0.493519
aF0.495062
aF0.491376
aF0.491692
aF0.492392
aF0.492772
aF0.494827
aF0.489678
aF0.49658
aF0.488461
aF0.487947
atp63
tp64
Rp65
aa(lp66
S'loss_coverage'
p67
ag0
(g22
g23
(S'EuclideanLoss'
p68
(lp69
F131.104
aF21.4904
aF21.4202
aF21.3883
aF21.6455
aF21.4276
aF21.4244
aF21.4687
aF21.4537
aF21.3877
aF21.6412
aF21.423
aF15.9643
aF13.6789
aF13.2288
aF12.5975
aF12.0232
aF11.6276
aF10.8203
aF9.98635
aF9.76888
aF9.47847
aF9.40856
aF9.13632
aF8.99949
aF9.18939
aF8.98193
aF9.04268
aF8.91291
aF8.93978
aF8.88865
aF8.8788
aF8.79268
aF8.74083
aF8.75595
aF8.69846
aF8.69211
aF8.7089
aF8.73413
aF8.78444
aF8.79207
aF8.74522
aF8.88494
aF8.71966
aF8.76311
aF8.8456
aF8.68805
aF8.65489
aF8.70151
aF8.68153
aF8.73727
aF8.74076
aF8.68074
aF8.81083
aF8.70466
aF8.72772
aF8.79729
aF8.91834
aF8.87064
aF8.83542
aF8.91908
aF8.8189
aF8.82661
aF8.71563
aF8.71507
aF8.7414
aF8.75131
aF8.69978
aF8.76349
aF8.7286
aF8.84076
aF8.85184
aF8.83371
aF8.72516
aF8.75135
aF8.80201
aF8.75094
aF8.79168
aF8.76333
aF8.80273
aF8.77763
aF8.75349
aF8.86073
aF8.81745
aF8.7358
aF8.8555
aF8.75727
aF8.84883
aF8.71838
aF8.88437
aF8.79898
aF8.78621
aF8.79523
aF8.77598
aF8.79288
aF8.87474
aF8.7746
aF8.76034
aF8.84361
aF8.76418
aF8.77595
atp70
tp71
Rp72
aa(lp73
S'mAP'
p74
ag0
(g22
g23
(S'Python'
p75
(lp76
F0.0
aF0.0
aF0.0
aF0.0
aF0.0
aF0.0
aF0.0
aF0.0
aF0.0
aF0.0
aF0.0
aF0.0
aF4.61843
aF9.06228
aF9.38905
aF11.0521
aF10.0101
aF12.8652
aF16.5086
aF15.2386
aF15.6754
aF15.5833
aF17.0449
aF18.185
aF18.1335
aF18.3415
aF18.6991
aF20.4187
aF19.3183
aF18.947
aF18.9389
aF19.0247
aF18.119
aF17.964
aF18.5509
aF17.307
aF18.4637
aF16.792
aF18.9741
aF16.998
aF19.5414
aF17.1953
aF18.3806
aF17.3423
aF18.5806
aF16.8104
aF17.6031
aF17.018
aF17.6785
aF17.3726
aF17.0835
aF18.0517
aF17.7392
aF17.6427
aF17.4323
aF18.2599
aF17.5517
aF18.165
aF17.4765
aF18.8646
aF18.8976
aF18.1411
aF18.3758
aF18.2949
aF18.4443
aF18.4151
aF19.2541
aF19.1775
aF19.3452
aF18.4723
aF18.9722
aF19.1254
aF18.9976
aF18.3726
aF18.5805
aF18.3344
aF18.6798
aF19.1237
aF19.2886
aF19.4694
aF19.6596
aF19.2315
aF18.838
aF19.0926
aF19.1379
aF19.3525
aF19.363
aF20.225
aF19.189
aF19.7233
aF19.3842
aF19.8191
aF19.8046
aF19.2495
aF19.4435
aF20.1803
aF19.2552
aF20.2303
aF19.7694
aF20.5983
aF19.8302
atp77
tp78
Rp79
aa(lp80
S'precision'
p81
ag0
(g22
g23
(S'Python'
p82
(lp83
F0.0
aF0.0
aF0.0
aF0.0
aF0.0
aF0.0
aF0.0
aF0.0
aF0.0
aF0.0
aF0.0
aF0.0
aF10.7083
aF16.1169
aF17.0877
aF19.9667
aF18.9755
aF23.4909
aF34.4258
aF32.1332
aF35.2263
aF33.3227
aF36.7951
aF35.9503
aF37.5043
aF33.5345
aF35.7466
aF35.4994
aF34.093
aF32.8012
aF33.0866
aF32.0222
aF31.5795
aF30.7295
aF31.5859
aF28.6942
aF31.0162
aF28.1897
aF30.9774
aF27.5397
aF31.5258
aF28.0581
aF30.3537
aF27.2304
aF30.0798
aF26.3208
aF28.4726
aF26.4109
aF27.7078
aF26.6165
aF27.1179
aF27.1846
aF27.7947
aF27.5326
aF27.214
aF27.2956
aF27.6332
aF27.3695
aF27.6364
aF28.079
aF28.9148
aF27.0665
aF28.6646
aF27.3512
aF28.1728
aF27.5949
aF29.6631
aF27.8984
aF29.3979
aF27.1375
aF28.4051
aF28.2694
aF28.8003
aF27.288
aF28.2048
aF27.6271
aF27.6431
aF28.2707
aF28.867
aF28.6154
aF29.2951
aF28.1818
aF28.0731
aF28.5589
aF28.3294
aF28.6624
aF28.642
aF29.4159
aF28.2125
aF29.1999
aF28.6881
aF28.4931
aF28.9514
aF27.7128
aF28.8136
aF29.6754
aF28.5773
aF28.6134
aF29.2655
aF29.1933
aF28.6556
atp84
tp85
Rp86
aa(lp87
S'recall'
p88
ag0
(g22
g23
(S'Python'
p89
(lp90
F0.0
aF0.0
aF0.0
aF0.0
aF0.0
aF0.0
aF0.0
aF0.0
aF0.0
aF0.0
aF0.0
aF0.0
aF14.7381
aF30.2157
aF32.496
aF31.5761
aF35.0508
aF36.7391
aF30.3815
aF27.3941
aF27.5414
aF26.3137
aF30.0758
aF32.8481
aF32.7203
aF36.6315
aF36.655
aF39.944
aF39.0506
aF40.8788
aF40.2089
aF43.2596
aF42.114
aF43.3508
aF42.9666
aF45.7708
aF43.3676
aF45.7983
aF44.854
aF46.3989
aF45.9941
aF46.2185
aF43.4104
aF48.2003
aF46.6197
aF45.6674
aF48.0519
aF47.7903
aF48.6775
aF47.8455
aF47.6348
aF48.3509
aF48.9229
aF46.8453
aF49.362
aF49.5824
aF48.1582
aF48.197
aF47.2082
aF49.2756
aF49.7432
aF48.9322
aF48.4809
aF48.7991
aF48.5606
aF49.8329
aF50.3138
aF51.5348
aF50.0312
aF50.1525
aF51.0504
aF50.2387
aF50.1029
aF49.6371
aF50.4627
aF48.7328
aF52.1859
aF50.5239
aF51.0715
aF51.0358
aF52.2286
aF51.8965
aF50.6578
aF49.7535
aF52.4091
aF51.6251
aF51.999
aF52.4788
aF53.5177
aF51.5081
aF52.1223
aF53.1556
aF53.4513
aF52.7951
aF52.9613
aF51.6228
aF52.6719
aF54.6749
aF51.6203
aF54.5507
aF54.4944
atp91
tp92
Rp93
aatp94
Rp95
sS'loaded_snapshot_epoch'
p96
NsS'pretrained_model'
p97
V/workspace/jobs/bvlc_googlenet.caffemodel
p98
sS'deploy_file'
p99
S'deploy.prototxt'
p100
sS'current_iteration'
p101
I35900
sS'crop_size'
p102
NsS'rms_decay'
p103
F0.99
sS'traces_interval'
p104
I0
sS'train_epochs'
p105
I100
sS'network'
p106
ccaffe_pb2
NetParameter
p107
(tRp108
(dp109
S'serialized'
p110
S'\n\tDetectNet\xa2\x06K\n\ntrain_data\x12\x04Data"\x04dataB\x02\x08\x00\xda\x06,\n&examples/kitti/kitti_train_images.lmdb \n@\x01\xa2\x06M\n\x0btrain_label\x12\x04Data"\x05labelB\x02\x08\x00\xda\x06,\n&examples/kitti/kitti_train_labels.lmdb \n@\x01\xa2\x06M\n\x08val_data\x12\x04Data"\x04dataB\x07\x08\x01"\x03val\xda\x06+\n%examples/kitti/kitti_test_images.lmdb \x06@\x01\xa2\x06O\n\tval_label\x12\x04Data"\x05labelB\x07\x08\x01"\x03val\xda\x06+\n%examples/kitti/kitti_test_labels.lmdb \x06@\x01\xa2\x060\n\x0bdeploy_data\x12\x05Input"\x04dataB\x07\x08\x01*\x03val\xfa\x08\n\n\x08\n\x06\x01\x03\x80\x05\x80\x05\xa2\x06\xc8\x01\n\x0ftrain_transform\x12\x17DetectNetTransformation\x1a\x04data\x1a\x05label"\x10transformed_data"\x11transformed_labelB\x02\x08\x00\xa2\x06\x05-\x00\x00\xfeB\x8a\xa9\x03\x1d\x08\x10\x15\xcd\xcc\xcc>\x18\x01(\x148\x00@\x80\x05H\x80\x05X\x01`\x00j\x04\x08\x01\x10\x00\x92\xa9\x03;\r\x00\x00\x80?\x10 \x18 %\xcd\xcc\xcc>-\xcd\xccL?5\x9a\x99\x99?=\x00\x00\x00?E\x00\x00\x00\x00M\x00\x00\xa0@U\xcd\xccL?]\x00\x00\xf0Ae\xcd\xccL?m\xcd\xccL?\xa2\x06\x8c\x01\n\rval_transform\x12\x17DetectNetTransformation\x1a\x04data\x1a\x05label"\x10transformed_data"\x11transformed_labelB\x07\x08\x01"\x03val\xa2\x06\x05-\x00\x00\xfeB\x8a\xa9\x03\x1d\x08\x10\x15\xcd\xcc\xcc>\x18\x01(\x148\x00@\x80\x05H\x80\x05X\x01`\x00j\x04\x08\x01\x10\x00\xa2\x06B\n\x10deploy_transform\x12\x05Power\x1a\x04data"\x10transformed_dataB\x07\x08\x01*\x03val\xd2\x07\x05\x1d\x00\x00\xfe\xc2\xa2\x06\x86\x01\n\x0bslice-label\x12\x05Slice\x1a\x11transformed_label"\x10foreground-label"\nbbox-label"\nsize-label"\tobj-label"\x0ecoverage-labelB\x02\x08\x00B\x07\x08\x01"\x03val\xf2\x07\n\x08\x01\x10\x01\x10\x05\x10\x07\x10\x08\xa2\x06\x82\x01\n\x0ecoverage-block\x12\x06Concat\x1a\x10foreground-label\x1a\x10foreground-label\x1a\x10foreground-label\x1a\x10foreground-label"\x0ecoverage-blockB\x02\x08\x00B\x07\x08\x01"\x03val\xc2\x06\x02\x08\x01\xa2\x06J\n\nsize-block\x12\x06Concat\x1a\nsize-label\x1a\nsize-label"\nsize-blockB\x02\x08\x00B\x07\x08\x01"\x03val\xc2\x06\x02\x08\x01\xa2\x06\\\n\tobj-block\x12\x06Concat\x1a\tobj-label\x1a\tobj-label\x1a\tobj-label\x1a\tobj-label"\tobj-blockB\x02\x08\x00B\x07\x08\x01"\x03val\xc2\x06\x02\x08\x01\xa2\x06S\n\rbb-label-norm\x12\x07Eltwise\x1a\nbbox-label\x1a\nsize-block"\x0fbbox-label-normB\x02\x08\x00B\x07\x08\x01"\x03val\xf2\x06\x02\x08\x00\xa2\x06Y\n\x0bbb-obj-norm\x12\x07Eltwise\x1a\x0fbbox-label-norm\x1a\tobj-block"\x13bbox-obj-label-normB\x02\x08\x00B\x07\x08\x01"\x03val\xf2\x06\x02\x08\x00\xa2\x06~\n\x0cconv1/7x7_s2\x12\x0bConvolution\x1a\x10transformed_data"\x0cconv1/7x7_s22\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06(\x08@\x18\x03 \x070\x02:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x062\n\x0econv1/relu_7x7\x12\x04ReLU\x1a\x0cconv1/7x7_s2"\x0cconv1/7x7_s2\xa2\x06<\n\x0cpool1/3x3_s2\x12\x07Pooling\x1a\x0cconv1/7x7_s2"\x0cpool1/3x3_s2\xca\x07\x06\x08\x00\x10\x03\x18\x02\xa2\x06<\n\x0bpool1/norm1\x12\x03LRN\x1a\x0cpool1/3x3_s2"\x0bpool1/norm1\xb2\x07\x0c\x08\x05\x15\x17\xb7\xd18\x1d\x00\x00@?\xa2\x06}\n\x10conv2/3x3_reduce\x12\x0bConvolution\x1a\x0bpool1/norm1"\x10conv2/3x3_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08@ \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15conv2/relu_3x3_reduce\x12\x04ReLU\x1a\x10conv2/3x3_reduce"\x10conv2/3x3_reduce\xa2\x06w\n\tconv2/3x3\x12\x0bConvolution\x1a\x10conv2/3x3_reduce"\tconv2/3x32\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\xc0\x01\x18\x01 \x03:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06,\n\x0econv2/relu_3x3\x12\x04ReLU\x1a\tconv2/3x3"\tconv2/3x3\xa2\x069\n\x0bconv2/norm2\x12\x03LRN\x1a\tconv2/3x3"\x0bconv2/norm2\xb2\x07\x0c\x08\x05\x15\x17\xb7\xd18\x1d\x00\x00@?\xa2\x06;\n\x0cpool2/3x3_s2\x12\x07Pooling\x1a\x0bconv2/norm2"\x0cpool2/3x3_s2\xca\x07\x06\x08\x00\x10\x03\x18\x02\xa2\x06~\n\x10inception_3a/1x1\x12\x0bConvolution\x1a\x0cpool2/3x3_s2"\x10inception_3a/1x12\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08@ \x01:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_3a/relu_1x1\x12\x04ReLU\x1a\x10inception_3a/1x1"\x10inception_3a/1x1\xa2\x06\x8c\x01\n\x17inception_3a/3x3_reduce\x12\x0bConvolution\x1a\x0cpool2/3x3_s2"\x17inception_3a/3x3_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08` \x01:\r\n\x06xavier5\xecQ\xb8=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_3a/relu_3x3_reduce\x12\x04ReLU\x1a\x17inception_3a/3x3_reduce"\x17inception_3a/3x3_reduce\xa2\x06\x8c\x01\n\x10inception_3a/3x3\x12\x0bConvolution\x1a\x17inception_3a/3x3_reduce"\x10inception_3a/3x32\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\x80\x01\x18\x01 \x03:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_3a/relu_3x3\x12\x04ReLU\x1a\x10inception_3a/3x3"\x10inception_3a/3x3\xa2\x06\x8c\x01\n\x17inception_3a/5x5_reduce\x12\x0bConvolution\x1a\x0cpool2/3x3_s2"\x17inception_3a/5x5_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08\x10 \x01:\r\n\x06xavier5\xcd\xccL>B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_3a/relu_5x5_reduce\x12\x04ReLU\x1a\x17inception_3a/5x5_reduce"\x17inception_3a/5x5_reduce\xa2\x06\x8b\x01\n\x10inception_3a/5x5\x12\x0bConvolution\x1a\x17inception_3a/5x5_reduce"\x10inception_3a/5x52\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06&\x08 \x18\x02 \x05:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_3a/relu_5x5\x12\x04ReLU\x1a\x10inception_3a/5x5"\x10inception_3a/5x5\xa2\x06H\n\x11inception_3a/pool\x12\x07Pooling\x1a\x0cpool2/3x3_s2"\x11inception_3a/pool\xca\x07\x08\x08\x00\x10\x03\x18\x01 \x01\xa2\x06\x8f\x01\n\x16inception_3a/pool_proj\x12\x0bConvolution\x1a\x11inception_3a/pool"\x16inception_3a/pool_proj2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08  \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06S\n\x1binception_3a/relu_pool_proj\x12\x04ReLU\x1a\x16inception_3a/pool_proj"\x16inception_3a/pool_proj\xa2\x06\x80\x01\n\x13inception_3a/output\x12\x06Concat\x1a\x10inception_3a/1x1\x1a\x10inception_3a/3x3\x1a\x10inception_3a/5x5\x1a\x16inception_3a/pool_proj"\x13inception_3a/output\xa2\x06\x86\x01\n\x10inception_3b/1x1\x12\x0bConvolution\x1a\x13inception_3a/output"\x10inception_3b/1x12\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\x80\x01 \x01:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_3b/relu_1x1\x12\x04ReLU\x1a\x10inception_3b/1x1"\x10inception_3b/1x1\xa2\x06\x94\x01\n\x17inception_3b/3x3_reduce\x12\x0bConvolution\x1a\x13inception_3a/output"\x17inception_3b/3x3_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\x80\x01 \x01:\r\n\x06xavier5\xecQ\xb8=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_3b/relu_3x3_reduce\x12\x04ReLU\x1a\x17inception_3b/3x3_reduce"\x17inception_3b/3x3_reduce\xa2\x06\x8c\x01\n\x10inception_3b/3x3\x12\x0bConvolution\x1a\x17inception_3b/3x3_reduce"\x10inception_3b/3x32\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\xc0\x01\x18\x01 \x03:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_3b/relu_3x3\x12\x04ReLU\x1a\x10inception_3b/3x3"\x10inception_3b/3x3\xa2\x06\x93\x01\n\x17inception_3b/5x5_reduce\x12\x0bConvolution\x1a\x13inception_3a/output"\x17inception_3b/5x5_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08  \x01:\r\n\x06xavier5\xcd\xccL>B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_3b/relu_5x5_reduce\x12\x04ReLU\x1a\x17inception_3b/5x5_reduce"\x17inception_3b/5x5_reduce\xa2\x06\x8b\x01\n\x10inception_3b/5x5\x12\x0bConvolution\x1a\x17inception_3b/5x5_reduce"\x10inception_3b/5x52\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06&\x08`\x18\x02 \x05:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_3b/relu_5x5\x12\x04ReLU\x1a\x10inception_3b/5x5"\x10inception_3b/5x5\xa2\x06O\n\x11inception_3b/pool\x12\x07Pooling\x1a\x13inception_3a/output"\x11inception_3b/pool\xca\x07\x08\x08\x00\x10\x03\x18\x01 \x01\xa2\x06\x8f\x01\n\x16inception_3b/pool_proj\x12\x0bConvolution\x1a\x11inception_3b/pool"\x16inception_3b/pool_proj2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08@ \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06S\n\x1binception_3b/relu_pool_proj\x12\x04ReLU\x1a\x16inception_3b/pool_proj"\x16inception_3b/pool_proj\xa2\x06\x80\x01\n\x13inception_3b/output\x12\x06Concat\x1a\x10inception_3b/1x1\x1a\x10inception_3b/3x3\x1a\x10inception_3b/5x5\x1a\x16inception_3b/pool_proj"\x13inception_3b/output\xa2\x06C\n\x0cpool3/3x3_s2\x12\x07Pooling\x1a\x13inception_3b/output"\x0cpool3/3x3_s2\xca\x07\x06\x08\x00\x10\x03\x18\x02\xa2\x06\x7f\n\x10inception_4a/1x1\x12\x0bConvolution\x1a\x0cpool3/3x3_s2"\x10inception_4a/1x12\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\xc0\x01 \x01:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4a/relu_1x1\x12\x04ReLU\x1a\x10inception_4a/1x1"\x10inception_4a/1x1\xa2\x06\x8c\x01\n\x17inception_4a/3x3_reduce\x12\x0bConvolution\x1a\x0cpool3/3x3_s2"\x17inception_4a/3x3_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08` \x01:\r\n\x06xavier5\xecQ\xb8=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_4a/relu_3x3_reduce\x12\x04ReLU\x1a\x17inception_4a/3x3_reduce"\x17inception_4a/3x3_reduce\xa2\x06\x8c\x01\n\x10inception_4a/3x3\x12\x0bConvolution\x1a\x17inception_4a/3x3_reduce"\x10inception_4a/3x32\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\xd0\x01\x18\x01 \x03:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4a/relu_3x3\x12\x04ReLU\x1a\x10inception_4a/3x3"\x10inception_4a/3x3\xa2\x06\x8c\x01\n\x17inception_4a/5x5_reduce\x12\x0bConvolution\x1a\x0cpool3/3x3_s2"\x17inception_4a/5x5_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08\x10 \x01:\r\n\x06xavier5\xcd\xccL>B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_4a/relu_5x5_reduce\x12\x04ReLU\x1a\x17inception_4a/5x5_reduce"\x17inception_4a/5x5_reduce\xa2\x06\x8b\x01\n\x10inception_4a/5x5\x12\x0bConvolution\x1a\x17inception_4a/5x5_reduce"\x10inception_4a/5x52\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06&\x080\x18\x02 \x05:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4a/relu_5x5\x12\x04ReLU\x1a\x10inception_4a/5x5"\x10inception_4a/5x5\xa2\x06H\n\x11inception_4a/pool\x12\x07Pooling\x1a\x0cpool3/3x3_s2"\x11inception_4a/pool\xca\x07\x08\x08\x00\x10\x03\x18\x01 \x01\xa2\x06\x8f\x01\n\x16inception_4a/pool_proj\x12\x0bConvolution\x1a\x11inception_4a/pool"\x16inception_4a/pool_proj2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08@ \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06S\n\x1binception_4a/relu_pool_proj\x12\x04ReLU\x1a\x16inception_4a/pool_proj"\x16inception_4a/pool_proj\xa2\x06\x80\x01\n\x13inception_4a/output\x12\x06Concat\x1a\x10inception_4a/1x1\x1a\x10inception_4a/3x3\x1a\x10inception_4a/5x5\x1a\x16inception_4a/pool_proj"\x13inception_4a/output\xa2\x06\x86\x01\n\x10inception_4b/1x1\x12\x0bConvolution\x1a\x13inception_4a/output"\x10inception_4b/1x12\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\xa0\x01 \x01:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4b/relu_1x1\x12\x04ReLU\x1a\x10inception_4b/1x1"\x10inception_4b/1x1\xa2\x06\x93\x01\n\x17inception_4b/3x3_reduce\x12\x0bConvolution\x1a\x13inception_4a/output"\x17inception_4b/3x3_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08p \x01:\r\n\x06xavier5\xecQ\xb8=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_4b/relu_3x3_reduce\x12\x04ReLU\x1a\x17inception_4b/3x3_reduce"\x17inception_4b/3x3_reduce\xa2\x06\x8c\x01\n\x10inception_4b/3x3\x12\x0bConvolution\x1a\x17inception_4b/3x3_reduce"\x10inception_4b/3x32\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\xe0\x01\x18\x01 \x03:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4b/relu_3x3\x12\x04ReLU\x1a\x10inception_4b/3x3"\x10inception_4b/3x3\xa2\x06\x93\x01\n\x17inception_4b/5x5_reduce\x12\x0bConvolution\x1a\x13inception_4a/output"\x17inception_4b/5x5_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08\x18 \x01:\r\n\x06xavier5\xcd\xccL>B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_4b/relu_5x5_reduce\x12\x04ReLU\x1a\x17inception_4b/5x5_reduce"\x17inception_4b/5x5_reduce\xa2\x06\x8b\x01\n\x10inception_4b/5x5\x12\x0bConvolution\x1a\x17inception_4b/5x5_reduce"\x10inception_4b/5x52\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06&\x08@\x18\x02 \x05:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4b/relu_5x5\x12\x04ReLU\x1a\x10inception_4b/5x5"\x10inception_4b/5x5\xa2\x06O\n\x11inception_4b/pool\x12\x07Pooling\x1a\x13inception_4a/output"\x11inception_4b/pool\xca\x07\x08\x08\x00\x10\x03\x18\x01 \x01\xa2\x06\x8f\x01\n\x16inception_4b/pool_proj\x12\x0bConvolution\x1a\x11inception_4b/pool"\x16inception_4b/pool_proj2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08@ \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06S\n\x1binception_4b/relu_pool_proj\x12\x04ReLU\x1a\x16inception_4b/pool_proj"\x16inception_4b/pool_proj\xa2\x06\x80\x01\n\x13inception_4b/output\x12\x06Concat\x1a\x10inception_4b/1x1\x1a\x10inception_4b/3x3\x1a\x10inception_4b/5x5\x1a\x16inception_4b/pool_proj"\x13inception_4b/output\xa2\x06\x86\x01\n\x10inception_4c/1x1\x12\x0bConvolution\x1a\x13inception_4b/output"\x10inception_4c/1x12\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\x80\x01 \x01:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4c/relu_1x1\x12\x04ReLU\x1a\x10inception_4c/1x1"\x10inception_4c/1x1\xa2\x06\x94\x01\n\x17inception_4c/3x3_reduce\x12\x0bConvolution\x1a\x13inception_4b/output"\x17inception_4c/3x3_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\x80\x01 \x01:\r\n\x06xavier5\xecQ\xb8=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_4c/relu_3x3_reduce\x12\x04ReLU\x1a\x17inception_4c/3x3_reduce"\x17inception_4c/3x3_reduce\xa2\x06\x8c\x01\n\x10inception_4c/3x3\x12\x0bConvolution\x1a\x17inception_4c/3x3_reduce"\x10inception_4c/3x32\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\x80\x02\x18\x01 \x03:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4c/relu_3x3\x12\x04ReLU\x1a\x10inception_4c/3x3"\x10inception_4c/3x3\xa2\x06\x93\x01\n\x17inception_4c/5x5_reduce\x12\x0bConvolution\x1a\x13inception_4b/output"\x17inception_4c/5x5_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08\x18 \x01:\r\n\x06xavier5\xcd\xccL>B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_4c/relu_5x5_reduce\x12\x04ReLU\x1a\x17inception_4c/5x5_reduce"\x17inception_4c/5x5_reduce\xa2\x06\x8b\x01\n\x10inception_4c/5x5\x12\x0bConvolution\x1a\x17inception_4c/5x5_reduce"\x10inception_4c/5x52\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06&\x08@\x18\x02 \x05:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4c/relu_5x5\x12\x04ReLU\x1a\x10inception_4c/5x5"\x10inception_4c/5x5\xa2\x06O\n\x11inception_4c/pool\x12\x07Pooling\x1a\x13inception_4b/output"\x11inception_4c/pool\xca\x07\x08\x08\x00\x10\x03\x18\x01 \x01\xa2\x06\x8f\x01\n\x16inception_4c/pool_proj\x12\x0bConvolution\x1a\x11inception_4c/pool"\x16inception_4c/pool_proj2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08@ \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06S\n\x1binception_4c/relu_pool_proj\x12\x04ReLU\x1a\x16inception_4c/pool_proj"\x16inception_4c/pool_proj\xa2\x06\x80\x01\n\x13inception_4c/output\x12\x06Concat\x1a\x10inception_4c/1x1\x1a\x10inception_4c/3x3\x1a\x10inception_4c/5x5\x1a\x16inception_4c/pool_proj"\x13inception_4c/output\xa2\x06\x85\x01\n\x10inception_4d/1x1\x12\x0bConvolution\x1a\x13inception_4c/output"\x10inception_4d/1x12\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08p \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4d/relu_1x1\x12\x04ReLU\x1a\x10inception_4d/1x1"\x10inception_4d/1x1\xa2\x06\x94\x01\n\x17inception_4d/3x3_reduce\x12\x0bConvolution\x1a\x13inception_4c/output"\x17inception_4d/3x3_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\x90\x01 \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_4d/relu_3x3_reduce\x12\x04ReLU\x1a\x17inception_4d/3x3_reduce"\x17inception_4d/3x3_reduce\xa2\x06\x8c\x01\n\x10inception_4d/3x3\x12\x0bConvolution\x1a\x17inception_4d/3x3_reduce"\x10inception_4d/3x32\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\xa0\x02\x18\x01 \x03:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4d/relu_3x3\x12\x04ReLU\x1a\x10inception_4d/3x3"\x10inception_4d/3x3\xa2\x06\x93\x01\n\x17inception_4d/5x5_reduce\x12\x0bConvolution\x1a\x13inception_4c/output"\x17inception_4d/5x5_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08  \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_4d/relu_5x5_reduce\x12\x04ReLU\x1a\x17inception_4d/5x5_reduce"\x17inception_4d/5x5_reduce\xa2\x06\x8b\x01\n\x10inception_4d/5x5\x12\x0bConvolution\x1a\x17inception_4d/5x5_reduce"\x10inception_4d/5x52\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06&\x08@\x18\x02 \x05:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4d/relu_5x5\x12\x04ReLU\x1a\x10inception_4d/5x5"\x10inception_4d/5x5\xa2\x06O\n\x11inception_4d/pool\x12\x07Pooling\x1a\x13inception_4c/output"\x11inception_4d/pool\xca\x07\x08\x08\x00\x10\x03\x18\x01 \x01\xa2\x06\x8f\x01\n\x16inception_4d/pool_proj\x12\x0bConvolution\x1a\x11inception_4d/pool"\x16inception_4d/pool_proj2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08@ \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06S\n\x1binception_4d/relu_pool_proj\x12\x04ReLU\x1a\x16inception_4d/pool_proj"\x16inception_4d/pool_proj\xa2\x06\x80\x01\n\x13inception_4d/output\x12\x06Concat\x1a\x10inception_4d/1x1\x1a\x10inception_4d/3x3\x1a\x10inception_4d/5x5\x1a\x16inception_4d/pool_proj"\x13inception_4d/output\xa2\x06\x86\x01\n\x10inception_4e/1x1\x12\x0bConvolution\x1a\x13inception_4d/output"\x10inception_4e/1x12\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\x80\x02 \x01:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4e/relu_1x1\x12\x04ReLU\x1a\x10inception_4e/1x1"\x10inception_4e/1x1\xa2\x06\x94\x01\n\x17inception_4e/3x3_reduce\x12\x0bConvolution\x1a\x13inception_4d/output"\x17inception_4e/3x3_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\xa0\x01 \x01:\r\n\x06xavier5\xecQ\xb8=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_4e/relu_3x3_reduce\x12\x04ReLU\x1a\x17inception_4e/3x3_reduce"\x17inception_4e/3x3_reduce\xa2\x06\x8c\x01\n\x10inception_4e/3x3\x12\x0bConvolution\x1a\x17inception_4e/3x3_reduce"\x10inception_4e/3x32\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\xc0\x02\x18\x01 \x03:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4e/relu_3x3\x12\x04ReLU\x1a\x10inception_4e/3x3"\x10inception_4e/3x3\xa2\x06\x93\x01\n\x17inception_4e/5x5_reduce\x12\x0bConvolution\x1a\x13inception_4d/output"\x17inception_4e/5x5_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08  \x01:\r\n\x06xavier5\xcd\xccL>B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_4e/relu_5x5_reduce\x12\x04ReLU\x1a\x17inception_4e/5x5_reduce"\x17inception_4e/5x5_reduce\xa2\x06\x8c\x01\n\x10inception_4e/5x5\x12\x0bConvolution\x1a\x17inception_4e/5x5_reduce"\x10inception_4e/5x52\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\x80\x01\x18\x02 \x05:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4e/relu_5x5\x12\x04ReLU\x1a\x10inception_4e/5x5"\x10inception_4e/5x5\xa2\x06O\n\x11inception_4e/pool\x12\x07Pooling\x1a\x13inception_4d/output"\x11inception_4e/pool\xca\x07\x08\x08\x00\x10\x03\x18\x01 \x01\xa2\x06\x90\x01\n\x16inception_4e/pool_proj\x12\x0bConvolution\x1a\x11inception_4e/pool"\x16inception_4e/pool_proj2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\x80\x01 \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06S\n\x1binception_4e/relu_pool_proj\x12\x04ReLU\x1a\x16inception_4e/pool_proj"\x16inception_4e/pool_proj\xa2\x06\x80\x01\n\x13inception_4e/output\x12\x06Concat\x1a\x10inception_4e/1x1\x1a\x10inception_4e/3x3\x1a\x10inception_4e/5x5\x1a\x16inception_4e/pool_proj"\x13inception_4e/output\xa2\x06\x86\x01\n\x10inception_5a/1x1\x12\x0bConvolution\x1a\x13inception_4e/output"\x10inception_5a/1x12\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\x80\x02 \x01:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_5a/relu_1x1\x12\x04ReLU\x1a\x10inception_5a/1x1"\x10inception_5a/1x1\xa2\x06\x94\x01\n\x17inception_5a/3x3_reduce\x12\x0bConvolution\x1a\x13inception_4e/output"\x17inception_5a/3x3_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\xa0\x01 \x01:\r\n\x06xavier5\xecQ\xb8=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_5a/relu_3x3_reduce\x12\x04ReLU\x1a\x17inception_5a/3x3_reduce"\x17inception_5a/3x3_reduce\xa2\x06\x8c\x01\n\x10inception_5a/3x3\x12\x0bConvolution\x1a\x17inception_5a/3x3_reduce"\x10inception_5a/3x32\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\xc0\x02\x18\x01 \x03:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_5a/relu_3x3\x12\x04ReLU\x1a\x10inception_5a/3x3"\x10inception_5a/3x3\xa2\x06\x93\x01\n\x17inception_5a/5x5_reduce\x12\x0bConvolution\x1a\x13inception_4e/output"\x17inception_5a/5x5_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08  \x01:\r\n\x06xavier5\xcd\xccL>B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_5a/relu_5x5_reduce\x12\x04ReLU\x1a\x17inception_5a/5x5_reduce"\x17inception_5a/5x5_reduce\xa2\x06\x8c\x01\n\x10inception_5a/5x5\x12\x0bConvolution\x1a\x17inception_5a/5x5_reduce"\x10inception_5a/5x52\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\x80\x01\x18\x02 \x05:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_5a/relu_5x5\x12\x04ReLU\x1a\x10inception_5a/5x5"\x10inception_5a/5x5\xa2\x06O\n\x11inception_5a/pool\x12\x07Pooling\x1a\x13inception_4e/output"\x11inception_5a/pool\xca\x07\x08\x08\x00\x10\x03\x18\x01 \x01\xa2\x06\x90\x01\n\x16inception_5a/pool_proj\x12\x0bConvolution\x1a\x11inception_5a/pool"\x16inception_5a/pool_proj2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\x80\x01 \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06S\n\x1binception_5a/relu_pool_proj\x12\x04ReLU\x1a\x16inception_5a/pool_proj"\x16inception_5a/pool_proj\xa2\x06\x80\x01\n\x13inception_5a/output\x12\x06Concat\x1a\x10inception_5a/1x1\x1a\x10inception_5a/3x3\x1a\x10inception_5a/5x5\x1a\x16inception_5a/pool_proj"\x13inception_5a/output\xa2\x06\x86\x01\n\x10inception_5b/1x1\x12\x0bConvolution\x1a\x13inception_5a/output"\x10inception_5b/1x12\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\x80\x03 \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_5b/relu_1x1\x12\x04ReLU\x1a\x10inception_5b/1x1"\x10inception_5b/1x1\xa2\x06\x94\x01\n\x17inception_5b/3x3_reduce\x12\x0bConvolution\x1a\x13inception_5a/output"\x17inception_5b/3x3_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x80?%\x00\x00\x00\x00\xd2\x06%\x08\xc0\x01 \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_5b/relu_3x3_reduce\x12\x04ReLU\x1a\x17inception_5b/3x3_reduce"\x17inception_5b/3x3_reduce\xa2\x06\x8c\x01\n\x10inception_5b/3x3\x12\x0bConvolution\x1a\x17inception_5b/3x3_reduce"\x10inception_5b/3x32\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\x80\x03\x18\x01 \x03:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_5b/relu_3x3\x12\x04ReLU\x1a\x10inception_5b/3x3"\x10inception_5b/3x3\xa2\x06\x93\x01\n\x17inception_5b/5x5_reduce\x12\x0bConvolution\x1a\x13inception_5a/output"\x17inception_5b/5x5_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x080 \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_5b/relu_5x5_reduce\x12\x04ReLU\x1a\x17inception_5b/5x5_reduce"\x17inception_5b/5x5_reduce\xa2\x06\x8c\x01\n\x10inception_5b/5x5\x12\x0bConvolution\x1a\x17inception_5b/5x5_reduce"\x10inception_5b/5x52\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\x80\x01\x18\x02 \x05:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_5b/relu_5x5\x12\x04ReLU\x1a\x10inception_5b/5x5"\x10inception_5b/5x5\xa2\x06O\n\x11inception_5b/pool\x12\x07Pooling\x1a\x13inception_5a/output"\x11inception_5b/pool\xca\x07\x08\x08\x00\x10\x03\x18\x01 \x01\xa2\x06\x90\x01\n\x16inception_5b/pool_proj\x12\x0bConvolution\x1a\x11inception_5b/pool"\x16inception_5b/pool_proj2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\x80\x01 \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06S\n\x1binception_5b/relu_pool_proj\x12\x04ReLU\x1a\x16inception_5b/pool_proj"\x16inception_5b/pool_proj\xa2\x06\x80\x01\n\x13inception_5b/output\x12\x06Concat\x1a\x10inception_5b/1x1\x1a\x10inception_5b/3x3\x1a\x10inception_5b/5x5\x1a\x16inception_5b/pool_proj"\x13inception_5b/output\xa2\x06D\n\rpool5/drop_s1\x12\x07Dropout\x1a\x13inception_5b/output"\rpool5/drop_s1\xe2\x06\x05\r\xcd\xcc\xcc>\xa2\x06{\n\x0ecvg/classifier\x12\x0bConvolution\x1a\rpool5/drop_s1"\x0ecvg/classifier2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08\x01 \x01:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\x00\x00\x00\x00\xa2\x061\n\x0ccoverage/sig\x12\x07Sigmoid\x1a\x0ecvg/classifier"\x08coverage\xa2\x06s\n\x0ebbox/regressor\x12\x0bConvolution\x1a\rpool5/drop_s1"\x06bboxes2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08\x04 \x01:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\x00\x00\x00\x00\xa2\x06M\n\tbbox_mask\x12\x07Eltwise\x1a\x06bboxes\x1a\x0ecoverage-block"\rbboxes-maskedB\x02\x08\x00B\x07\x08\x01"\x03val\xf2\x06\x02\x08\x00\xa2\x06U\n\tbbox-norm\x12\x07Eltwise\x1a\rbboxes-masked\x1a\nsize-block"\x12bboxes-masked-normB\x02\x08\x00B\x07\x08\x01"\x03val\xf2\x06\x02\x08\x00\xa2\x06a\n\rbbox-obj-norm\x12\x07Eltwise\x1a\x12bboxes-masked-norm\x1a\tobj-block"\x16bboxes-obj-masked-normB\x02\x08\x00B\x07\x08\x01"\x03val\xf2\x06\x02\x08\x00\xa2\x06]\n\tbbox_loss\x12\x06L1Loss\x1a\x16bboxes-obj-masked-norm\x1a\x13bbox-obj-label-norm"\tloss_bbox-\x00\x00\x00@B\x02\x08\x00B\x07\x08\x01"\x03val\xa2\x06T\n\rcoverage_loss\x12\rEuclideanLoss\x1a\x08coverage\x1a\x0ecoverage-label"\rloss_coverageB\x02\x08\x00B\x07\x08\x01"\x03val\xa2\x06\x8e\x01\n\x07cluster\x12\x06Python\x1a\x08coverage\x1a\x06bboxes"\tbbox-listB\x02\x08\x01\x92\x08Y\n!caffe.layers.detectnet.clustering\x12\x11ClusterDetections\x1a!640, 640, 16, 0.6, 2, 0.02, 22, 1\xa2\x06\x95\x01\n\ncluster_gt\x12\x06Python\x1a\x0ecoverage-label\x1a\nbbox-label"\x0fbbox-list-labelB\x07\x08\x01"\x03val\x92\x08H\n!caffe.layers.detectnet.clustering\x12\x12ClusterGroundtruth\x1a\x0f640, 640, 16, 1\xa2\x06z\n\x05score\x12\x06Python\x1a\x0fbbox-list-label\x1a\tbbox-list"\x10bbox-list-scoredB\x07\x08\x01"\x03val\x92\x081\n\x1ecaffe.layers.detectnet.mean_ap\x12\x0fScoreDetections\xa2\x06v\n\x03mAP\x12\x06Python\x1a\x10bbox-list-scored"\x03mAP"\tprecision"\x06recallB\x07\x08\x01"\x03val\x92\x083\n\x1ecaffe.layers.detectnet.mean_ap\x12\x03mAP\x1a\x0c640, 640, 16'
p111
sbsS'use_mean'
p112
Vnone
p113
sS'model_file'
p114
S'original.prototxt'
p115
sS'job_dir'
p116
S'/workspace/jobs/20181130-223734-9d03'
p117
sS'receiving_val_output'
p118
I00
sS'parents'
p119
NsS'blob_format'
p120
VNVCaffe
p121
sS'timeline_traces'
p122
(lp123
sS'gpu_count'
p124
I1
sS'receiving_train_output'
p125
I00
sS'val_interval'
p126
F1.0
sS'current_epoch'
p127
F100.0
sS'random_seed'
p128
NsS'saving_snapshot'
p129
I00
sS'pickver_task_caffe_train'
p130
I5
sg44
F2.5e-05
sS'loaded_snapshot_file'
p131
NsS'batch_size'
p132
NsS'digits_version'
p133
S'6.1.1'
p134
sS'job'
p135
g4
sS'progress'
p136
F1.0
sS'current_resources'
p137
NsS'pickver_task'
p138
I1
sS'lr_policy'
p139
(dp140
S'policy'
p141
Vexp
p142
sS'gamma'
p143
F0.97
ssS'job_id'
p144
S'20181130-223734-9d03'
p145
sS'last_train_update'
p146
F1543623574.042093
sS'exception'
p147
NsS'data_aug'
p148
(dp149
S'scale'
p150
F0.0
sS'hsv_use'
p151
I00
sS'flip'
p152
Vnone
p153
sS'quad_rot'
p154
Vnone
p155
sS'noise'
p156
F0.0
sS'whitening'
p157
I00
sS'hsv_h'
p158
F0.02
sS'hsv_s'
p159
F0.04
sS'rot'
p160
I0
sS'hsv_v'
p161
F0.06
sS'contrast'
p162
F0.0
ssS'selected_gpus'
p163
NsS'solver'
p164
ccaffe_pb2
SolverParameter
p165
(tRp166
(dp167
g110
S'\x18\xd4\x01 \xe7\x02-\x17\xb7\xd170,8\xbc\x98\x02B\x03expMq\xfa\x7f?]fff?e\xbd7\x864p\x86\x1cz\x08snapshot\x88\x01\x01\xc2\x01\x12train_val.prototxt\xf0\x01\x05'
p168
sbsS'snapshot_prefix'
p169
S'snapshot'
p170
sS'traceback'
p171
NsS'framework_id'
p172
S'caffe'
p173
sS'status_history'
p174
(lp175
((idigits.status
Status
p176
S'I'
p178
bF1543617454.788433
tp179
a((idigits.status
Status
p180
S'R'
p181
bF1543617456.456917
tp182
a((idigits.status
Status
p183
S'D'
p184
bF1543623597.620274
tp185
asS'pickver_task_train'
p186
I2
sS'train_val_file'
p187
S'train_val.prototxt'
p188
sS'image_mean'
p189
NsS'batch_accumulation'
p190
NsS'caffe_version'
p191
S'0.17.0'
p192
sS'caffe_flavor'
p193
S'NVIDIA'
p194
sS'log_file'
p195
S'caffe_output.log'
p196
sS'solver_file'
p197
S'solver.prototxt'
p198
sS'solver_type'
p199
VADAM
p200
sbasS'pickver_job_model_image_generic'
p201
I1
sS'pickver_job'
p202
I2
sS'dataset_id'
p203
S'20181130-223152-9c61'
p204
sS'_name'
p205
Vwasp J03
p206
sS'pickver_job_model_image'
p207
I1
sg147
NsS'group'
p208
Vwasp J03
p209
sS'persistent'
p210
I01
sS'form_data'
p211
(dp212
S'form.standard_networks.data'
p213
VNone
p214
sS'form.learning_rate.data'
p215
(lp216
F2.5e-05
asS'form.aug_flip.data'
p217
g153
sS'form.select_gpus.data'
p218
(lp219
sS'form.framework.data'
p220
Vcaffe
p221
sS'form.dataset.data'
p222
V20181130-223152-9c61
p223
sS'form.python_layer_from_client.data'
p224
I00
sS'form.lr_multistep_values.data'
p225
V50,85
p226
sS'form.aug_hsv_s.data'
p227
F0.04
sS'form.aug_hsv_h.data'
p228
F0.02
sS'form.lr_policy.data'
p229
g142
sS'form.previous_networks.data'
p230
VNone
p231
sS'form.python_layer_client_file.data'
p232
V
p233
sS'form.lr_sigmoid_gamma.data'
p234
F0.1
sS'form.aug_quad_rot.data'
p235
g155
sS'form.rms_decay.data'
p236
F0.99
sS'form.method.data'
p237
Vcustom
p238
sS'form.lr_sigmoid_step.data'
p239
F50.0
sS'form.nvcaffe_blob_format.data'
p240
g121
sS'form.crop_size.data'
p241
NsS'form.select_gpu_count.data'
p242
I1
sS'form.python_layer_server_file.data'
p243
g233
sS'form.custom_network.data'
p244
V# DetectNet network\u000a\u000a# Data/Input layers\u000aname: "DetectNet"\u000alayer {\u000a  name: "train_data"\u000a  type: "Data"\u000a  top: "data"\u000a  data_param {\u000a    backend: LMDB\u000a    source: "examples/kitti/kitti_train_images.lmdb"\u000a    batch_size: 10\u000a  }\u000a  include: { phase: TRAIN }\u000a}\u000alayer {\u000a  name: "train_label"\u000a  type: "Data"\u000a  top: "label"\u000a  data_param {\u000a    backend: LMDB\u000a    source: "examples/kitti/kitti_train_labels.lmdb"\u000a    batch_size: 10\u000a  }\u000a  include: { phase: TRAIN }\u000a}\u000alayer {\u000a  name: "val_data"\u000a  type: "Data"\u000a  top: "data"\u000a  data_param {\u000a    backend: LMDB\u000a    source: "examples/kitti/kitti_test_images.lmdb"\u000a    batch_size: 6\u000a  }\u000a  include: { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a  name: "val_label"\u000a  type: "Data"\u000a  top: "label"\u000a  data_param {\u000a    backend: LMDB\u000a    source: "examples/kitti/kitti_test_labels.lmdb"\u000a    batch_size: 6\u000a  }\u000a  include: { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a  name: "deploy_data"\u000a  type: "Input"\u000a  top: "data"\u000a  input_param {\u000a    shape {\u000a      dim: 1\u000a      dim: 3\u000a      dim: 640\u000a      dim: 640\u000a    }\u000a  }\u000a  include: { phase: TEST not_stage: "val" }\u000a}\u000a\u000a# Data transformation layers\u000alayer {\u000a  name: "train_transform"\u000a  type: "DetectNetTransformation"\u000a  bottom: "data"\u000a  bottom: "label"\u000a  top: "transformed_data"\u000a  top: "transformed_label"\u000a  detectnet_groundtruth_param: {\u000a    stride: 16\u000a    scale_cvg: 0.4\u000a    gridbox_type: GRIDBOX_MIN\u000a    coverage_type: RECTANGULAR\u000a    min_cvg_len: 20\u000a    obj_norm: true\u000a    image_size_x: 640\u000a    image_size_y: 640\u000a    crop_bboxes: false\u000a    object_class: { src: 1 dst: 0} # obj class 1 -> cvg index 0\u000a  }\u000a   detectnet_augmentation_param: {\u000a    crop_prob: 1\u000a    shift_x: 32\u000a    shift_y: 32\u000a    flip_prob: 0.5\u000a    rotation_prob: 0\u000a    max_rotate_degree: 5\u000a    scale_prob: 0.4\u000a    scale_min: 0.8\u000a    scale_max: 1.2\u000a    hue_rotation_prob: 0.8\u000a    hue_rotation: 30\u000a    desaturation_prob: 0.8\u000a    desaturation_max: 0.8\u000a  }\u000a  transform_param: {\u000a    mean_value: 127\u000a  }\u000a  include: { phase: TRAIN }\u000a}\u000alayer {\u000a  name: "val_transform"\u000a  type: "DetectNetTransformation"\u000a  bottom: "data"\u000a  bottom: "label"\u000a  top: "transformed_data"\u000a  top: "transformed_label"\u000a  detectnet_groundtruth_param: {\u000a    stride: 16\u000a    scale_cvg: 0.4\u000a    gridbox_type: GRIDBOX_MIN\u000a    coverage_type: RECTANGULAR\u000a    min_cvg_len: 20\u000a    obj_norm: true\u000a    image_size_x: 640\u000a    image_size_y: 640\u000a    crop_bboxes: false\u000a    object_class: { src: 1 dst: 0} # obj class 1 -> cvg index 0\u000a  }\u000a  transform_param: {\u000a    mean_value: 127\u000a  }\u000a  include: { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a  name: "deploy_transform"\u000a  type: "Power"\u000a  bottom: "data"\u000a  top: "transformed_data"\u000a  power_param {\u000a    shift: -127\u000a  }\u000a  include: { phase: TEST not_stage: "val" }\u000a}\u000a\u000a# Label conversion layers\u000alayer {\u000a  name: "slice-label"\u000a  type: "Slice"\u000a  bottom: "transformed_label"\u000a  top: "foreground-label"\u000a  top: "bbox-label"\u000a  top: "size-label"\u000a  top: "obj-label"\u000a  top: "coverage-label"\u000a  slice_param {\u000a    slice_dim: 1\u000a    slice_point: 1\u000a    slice_point: 5\u000a    slice_point: 7\u000a    slice_point: 8\u000a  }\u000a  include { phase: TRAIN }\u000a  include { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a  name: "coverage-block"\u000a  type: "Concat"\u000a  bottom: "foreground-label"\u000a  bottom: "foreground-label"\u000a  bottom: "foreground-label"\u000a  bottom: "foreground-label"\u000a  top: "coverage-block"\u000a  concat_param {\u000a    concat_dim: 1\u000a  }\u000a  include { phase: TRAIN }\u000a  include { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a  name: "size-block"\u000a  type: "Concat"\u000a  bottom: "size-label"\u000a  bottom: "size-label"\u000a  top: "size-block"\u000a  concat_param {\u000a    concat_dim: 1\u000a  }\u000a  include { phase: TRAIN }\u000a  include { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a  name: "obj-block"\u000a  type: "Concat"\u000a  bottom: "obj-label"\u000a  bottom: "obj-label"\u000a  bottom: "obj-label"\u000a  bottom: "obj-label"\u000a  top: "obj-block"\u000a  concat_param {\u000a    concat_dim: 1\u000a  }\u000a  include { phase: TRAIN }\u000a  include { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a  name: "bb-label-norm"\u000a  type: "Eltwise"\u000a  bottom: "bbox-label"\u000a  bottom: "size-block"\u000a  top: "bbox-label-norm"\u000a  eltwise_param {\u000a    operation: PROD\u000a  }\u000a  include { phase: TRAIN }\u000a  include { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a  name: "bb-obj-norm"\u000a  type: "Eltwise"\u000a  bottom: "bbox-label-norm"\u000a  bottom: "obj-block"\u000a  top: "bbox-obj-label-norm"\u000a  eltwise_param {\u000a    operation: PROD\u000a  }\u000a  include { phase: TRAIN }\u000a  include { phase: TEST stage: "val" }\u000a}\u000a\u000a######################################################################\u000a# Start of convolutional network\u000a######################################################################\u000a\u000alayer {\u000a  name: "conv1/7x7_s2"\u000a  type: "Convolution"\u000a  bottom: "transformed_data"\u000a  top: "conv1/7x7_s2"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 64\u000a    pad: 3\u000a    kernel_size: 7\u000a    stride: 2\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "conv1/relu_7x7"\u000a  type: "ReLU"\u000a  bottom: "conv1/7x7_s2"\u000a  top: "conv1/7x7_s2"\u000a}\u000a\u000alayer {\u000a  name: "pool1/3x3_s2"\u000a  type: "Pooling"\u000a  bottom: "conv1/7x7_s2"\u000a  top: "pool1/3x3_s2"\u000a  pooling_param {\u000a    pool: MAX\u000a    kernel_size: 3\u000a    stride: 2\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "pool1/norm1"\u000a  type: "LRN"\u000a  bottom: "pool1/3x3_s2"\u000a  top: "pool1/norm1"\u000a  lrn_param {\u000a    local_size: 5\u000a    alpha: 0.0001\u000a    beta: 0.75\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "conv2/3x3_reduce"\u000a  type: "Convolution"\u000a  bottom: "pool1/norm1"\u000a  top: "conv2/3x3_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 64\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "conv2/relu_3x3_reduce"\u000a  type: "ReLU"\u000a  bottom: "conv2/3x3_reduce"\u000a  top: "conv2/3x3_reduce"\u000a}\u000a\u000alayer {\u000a  name: "conv2/3x3"\u000a  type: "Convolution"\u000a  bottom: "conv2/3x3_reduce"\u000a  top: "conv2/3x3"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 192\u000a    pad: 1\u000a    kernel_size: 3\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "conv2/relu_3x3"\u000a  type: "ReLU"\u000a  bottom: "conv2/3x3"\u000a  top: "conv2/3x3"\u000a}\u000a\u000alayer {\u000a  name: "conv2/norm2"\u000a  type: "LRN"\u000a  bottom: "conv2/3x3"\u000a  top: "conv2/norm2"\u000a  lrn_param {\u000a    local_size: 5\u000a    alpha: 0.0001\u000a    beta: 0.75\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "pool2/3x3_s2"\u000a  type: "Pooling"\u000a  bottom: "conv2/norm2"\u000a  top: "pool2/3x3_s2"\u000a  pooling_param {\u000a    pool: MAX\u000a    kernel_size: 3\u000a    stride: 2\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_3a/1x1"\u000a  type: "Convolution"\u000a  bottom: "pool2/3x3_s2"\u000a  top: "inception_3a/1x1"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 64\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_3a/relu_1x1"\u000a  type: "ReLU"\u000a  bottom: "inception_3a/1x1"\u000a  top: "inception_3a/1x1"\u000a}\u000a\u000alayer {\u000a  name: "inception_3a/3x3_reduce"\u000a  type: "Convolution"\u000a  bottom: "pool2/3x3_s2"\u000a  top: "inception_3a/3x3_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 96\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.09\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_3a/relu_3x3_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_3a/3x3_reduce"\u000a  top: "inception_3a/3x3_reduce"\u000a}\u000a\u000alayer {\u000a  name: "inception_3a/3x3"\u000a  type: "Convolution"\u000a  bottom: "inception_3a/3x3_reduce"\u000a  top: "inception_3a/3x3"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 128\u000a    pad: 1\u000a    kernel_size: 3\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_3a/relu_3x3"\u000a  type: "ReLU"\u000a  bottom: "inception_3a/3x3"\u000a  top: "inception_3a/3x3"\u000a}\u000a\u000alayer {\u000a  name: "inception_3a/5x5_reduce"\u000a  type: "Convolution"\u000a  bottom: "pool2/3x3_s2"\u000a  top: "inception_3a/5x5_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 16\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.2\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_3a/relu_5x5_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_3a/5x5_reduce"\u000a  top: "inception_3a/5x5_reduce"\u000a}\u000alayer {\u000a  name: "inception_3a/5x5"\u000a  type: "Convolution"\u000a  bottom: "inception_3a/5x5_reduce"\u000a  top: "inception_3a/5x5"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 32\u000a    pad: 2\u000a    kernel_size: 5\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_3a/relu_5x5"\u000a  type: "ReLU"\u000a  bottom: "inception_3a/5x5"\u000a  top: "inception_3a/5x5"\u000a}\u000a\u000alayer {\u000a  name: "inception_3a/pool"\u000a  type: "Pooling"\u000a  bottom: "pool2/3x3_s2"\u000a  top: "inception_3a/pool"\u000a  pooling_param {\u000a    pool: MAX\u000a    kernel_size: 3\u000a    stride: 1\u000a    pad: 1\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_3a/pool_proj"\u000a  type: "Convolution"\u000a  bottom: "inception_3a/pool"\u000a  top: "inception_3a/pool_proj"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 32\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_3a/relu_pool_proj"\u000a  type: "ReLU"\u000a  bottom: "inception_3a/pool_proj"\u000a  top: "inception_3a/pool_proj"\u000a}\u000a\u000alayer {\u000a  name: "inception_3a/output"\u000a  type: "Concat"\u000a  bottom: "inception_3a/1x1"\u000a  bottom: "inception_3a/3x3"\u000a  bottom: "inception_3a/5x5"\u000a  bottom: "inception_3a/pool_proj"\u000a  top: "inception_3a/output"\u000a}\u000a\u000alayer {\u000a  name: "inception_3b/1x1"\u000a  type: "Convolution"\u000a  bottom: "inception_3a/output"\u000a  top: "inception_3b/1x1"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 128\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_3b/relu_1x1"\u000a  type: "ReLU"\u000a  bottom: "inception_3b/1x1"\u000a  top: "inception_3b/1x1"\u000a}\u000a\u000alayer {\u000a  name: "inception_3b/3x3_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_3a/output"\u000a  top: "inception_3b/3x3_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 128\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.09\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_3b/relu_3x3_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_3b/3x3_reduce"\u000a  top: "inception_3b/3x3_reduce"\u000a}\u000alayer {\u000a  name: "inception_3b/3x3"\u000a  type: "Convolution"\u000a  bottom: "inception_3b/3x3_reduce"\u000a  top: "inception_3b/3x3"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 192\u000a    pad: 1\u000a    kernel_size: 3\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_3b/relu_3x3"\u000a  type: "ReLU"\u000a  bottom: "inception_3b/3x3"\u000a  top: "inception_3b/3x3"\u000a}\u000a\u000alayer {\u000a  name: "inception_3b/5x5_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_3a/output"\u000a  top: "inception_3b/5x5_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 32\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.2\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_3b/relu_5x5_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_3b/5x5_reduce"\u000a  top: "inception_3b/5x5_reduce"\u000a}\u000alayer {\u000a  name: "inception_3b/5x5"\u000a  type: "Convolution"\u000a  bottom: "inception_3b/5x5_reduce"\u000a  top: "inception_3b/5x5"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 96\u000a    pad: 2\u000a    kernel_size: 5\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_3b/relu_5x5"\u000a  type: "ReLU"\u000a  bottom: "inception_3b/5x5"\u000a  top: "inception_3b/5x5"\u000a}\u000a\u000alayer {\u000a  name: "inception_3b/pool"\u000a  type: "Pooling"\u000a  bottom: "inception_3a/output"\u000a  top: "inception_3b/pool"\u000a  pooling_param {\u000a    pool: MAX\u000a    kernel_size: 3\u000a    stride: 1\u000a    pad: 1\u000a  }\u000a}\u000alayer {\u000a  name: "inception_3b/pool_proj"\u000a  type: "Convolution"\u000a  bottom: "inception_3b/pool"\u000a  top: "inception_3b/pool_proj"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 64\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_3b/relu_pool_proj"\u000a  type: "ReLU"\u000a  bottom: "inception_3b/pool_proj"\u000a  top: "inception_3b/pool_proj"\u000a}\u000alayer {\u000a  name: "inception_3b/output"\u000a  type: "Concat"\u000a  bottom: "inception_3b/1x1"\u000a  bottom: "inception_3b/3x3"\u000a  bottom: "inception_3b/5x5"\u000a  bottom: "inception_3b/pool_proj"\u000a  top: "inception_3b/output"\u000a}\u000a\u000alayer {\u000a  name: "pool3/3x3_s2"\u000a  type: "Pooling"\u000a  bottom: "inception_3b/output"\u000a  top: "pool3/3x3_s2"\u000a  pooling_param {\u000a    pool: MAX\u000a    kernel_size: 3\u000a    stride: 2\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_4a/1x1"\u000a  type: "Convolution"\u000a  bottom: "pool3/3x3_s2"\u000a  top: "inception_4a/1x1"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 192\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_4a/relu_1x1"\u000a  type: "ReLU"\u000a  bottom: "inception_4a/1x1"\u000a  top: "inception_4a/1x1"\u000a}\u000a\u000alayer {\u000a  name: "inception_4a/3x3_reduce"\u000a  type: "Convolution"\u000a  bottom: "pool3/3x3_s2"\u000a  top: "inception_4a/3x3_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 96\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.09\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_4a/relu_3x3_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_4a/3x3_reduce"\u000a  top: "inception_4a/3x3_reduce"\u000a}\u000a\u000alayer {\u000a  name: "inception_4a/3x3"\u000a  type: "Convolution"\u000a  bottom: "inception_4a/3x3_reduce"\u000a  top: "inception_4a/3x3"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 208\u000a    pad: 1\u000a    kernel_size: 3\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_4a/relu_3x3"\u000a  type: "ReLU"\u000a  bottom: "inception_4a/3x3"\u000a  top: "inception_4a/3x3"\u000a}\u000a\u000alayer {\u000a  name: "inception_4a/5x5_reduce"\u000a  type: "Convolution"\u000a  bottom: "pool3/3x3_s2"\u000a  top: "inception_4a/5x5_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 16\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.2\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4a/relu_5x5_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_4a/5x5_reduce"\u000a  top: "inception_4a/5x5_reduce"\u000a}\u000alayer {\u000a  name: "inception_4a/5x5"\u000a  type: "Convolution"\u000a  bottom: "inception_4a/5x5_reduce"\u000a  top: "inception_4a/5x5"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 48\u000a    pad: 2\u000a    kernel_size: 5\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4a/relu_5x5"\u000a  type: "ReLU"\u000a  bottom: "inception_4a/5x5"\u000a  top: "inception_4a/5x5"\u000a}\u000alayer {\u000a  name: "inception_4a/pool"\u000a  type: "Pooling"\u000a  bottom: "pool3/3x3_s2"\u000a  top: "inception_4a/pool"\u000a  pooling_param {\u000a    pool: MAX\u000a    kernel_size: 3\u000a    stride: 1\u000a    pad: 1\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4a/pool_proj"\u000a  type: "Convolution"\u000a  bottom: "inception_4a/pool"\u000a  top: "inception_4a/pool_proj"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 64\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4a/relu_pool_proj"\u000a  type: "ReLU"\u000a  bottom: "inception_4a/pool_proj"\u000a  top: "inception_4a/pool_proj"\u000a}\u000alayer {\u000a  name: "inception_4a/output"\u000a  type: "Concat"\u000a  bottom: "inception_4a/1x1"\u000a  bottom: "inception_4a/3x3"\u000a  bottom: "inception_4a/5x5"\u000a  bottom: "inception_4a/pool_proj"\u000a  top: "inception_4a/output"\u000a}\u000a\u000alayer {\u000a  name: "inception_4b/1x1"\u000a  type: "Convolution"\u000a  bottom: "inception_4a/output"\u000a  top: "inception_4b/1x1"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 160\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_4b/relu_1x1"\u000a  type: "ReLU"\u000a  bottom: "inception_4b/1x1"\u000a  top: "inception_4b/1x1"\u000a}\u000alayer {\u000a  name: "inception_4b/3x3_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_4a/output"\u000a  top: "inception_4b/3x3_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 112\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.09\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4b/relu_3x3_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_4b/3x3_reduce"\u000a  top: "inception_4b/3x3_reduce"\u000a}\u000alayer {\u000a  name: "inception_4b/3x3"\u000a  type: "Convolution"\u000a  bottom: "inception_4b/3x3_reduce"\u000a  top: "inception_4b/3x3"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 224\u000a    pad: 1\u000a    kernel_size: 3\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4b/relu_3x3"\u000a  type: "ReLU"\u000a  bottom: "inception_4b/3x3"\u000a  top: "inception_4b/3x3"\u000a}\u000alayer {\u000a  name: "inception_4b/5x5_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_4a/output"\u000a  top: "inception_4b/5x5_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 24\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.2\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4b/relu_5x5_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_4b/5x5_reduce"\u000a  top: "inception_4b/5x5_reduce"\u000a}\u000alayer {\u000a  name: "inception_4b/5x5"\u000a  type: "Convolution"\u000a  bottom: "inception_4b/5x5_reduce"\u000a  top: "inception_4b/5x5"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 64\u000a    pad: 2\u000a    kernel_size: 5\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4b/relu_5x5"\u000a  type: "ReLU"\u000a  bottom: "inception_4b/5x5"\u000a  top: "inception_4b/5x5"\u000a}\u000alayer {\u000a  name: "inception_4b/pool"\u000a  type: "Pooling"\u000a  bottom: "inception_4a/output"\u000a  top: "inception_4b/pool"\u000a  pooling_param {\u000a    pool: MAX\u000a    kernel_size: 3\u000a    stride: 1\u000a    pad: 1\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4b/pool_proj"\u000a  type: "Convolution"\u000a  bottom: "inception_4b/pool"\u000a  top: "inception_4b/pool_proj"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 64\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4b/relu_pool_proj"\u000a  type: "ReLU"\u000a  bottom: "inception_4b/pool_proj"\u000a  top: "inception_4b/pool_proj"\u000a}\u000alayer {\u000a  name: "inception_4b/output"\u000a  type: "Concat"\u000a  bottom: "inception_4b/1x1"\u000a  bottom: "inception_4b/3x3"\u000a  bottom: "inception_4b/5x5"\u000a  bottom: "inception_4b/pool_proj"\u000a  top: "inception_4b/output"\u000a}\u000a\u000alayer {\u000a  name: "inception_4c/1x1"\u000a  type: "Convolution"\u000a  bottom: "inception_4b/output"\u000a  top: "inception_4c/1x1"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 128\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_4c/relu_1x1"\u000a  type: "ReLU"\u000a  bottom: "inception_4c/1x1"\u000a  top: "inception_4c/1x1"\u000a}\u000a\u000alayer {\u000a  name: "inception_4c/3x3_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_4b/output"\u000a  top: "inception_4c/3x3_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 128\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.09\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_4c/relu_3x3_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_4c/3x3_reduce"\u000a  top: "inception_4c/3x3_reduce"\u000a}\u000alayer {\u000a  name: "inception_4c/3x3"\u000a  type: "Convolution"\u000a  bottom: "inception_4c/3x3_reduce"\u000a  top: "inception_4c/3x3"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 256\u000a    pad: 1\u000a    kernel_size: 3\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4c/relu_3x3"\u000a  type: "ReLU"\u000a  bottom: "inception_4c/3x3"\u000a  top: "inception_4c/3x3"\u000a}\u000alayer {\u000a  name: "inception_4c/5x5_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_4b/output"\u000a  top: "inception_4c/5x5_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 24\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.2\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4c/relu_5x5_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_4c/5x5_reduce"\u000a  top: "inception_4c/5x5_reduce"\u000a}\u000alayer {\u000a  name: "inception_4c/5x5"\u000a  type: "Convolution"\u000a  bottom: "inception_4c/5x5_reduce"\u000a  top: "inception_4c/5x5"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 64\u000a    pad: 2\u000a    kernel_size: 5\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4c/relu_5x5"\u000a  type: "ReLU"\u000a  bottom: "inception_4c/5x5"\u000a  top: "inception_4c/5x5"\u000a}\u000alayer {\u000a  name: "inception_4c/pool"\u000a  type: "Pooling"\u000a  bottom: "inception_4b/output"\u000a  top: "inception_4c/pool"\u000a  pooling_param {\u000a    pool: MAX\u000a    kernel_size: 3\u000a    stride: 1\u000a    pad: 1\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4c/pool_proj"\u000a  type: "Convolution"\u000a  bottom: "inception_4c/pool"\u000a  top: "inception_4c/pool_proj"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 64\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4c/relu_pool_proj"\u000a  type: "ReLU"\u000a  bottom: "inception_4c/pool_proj"\u000a  top: "inception_4c/pool_proj"\u000a}\u000alayer {\u000a  name: "inception_4c/output"\u000a  type: "Concat"\u000a  bottom: "inception_4c/1x1"\u000a  bottom: "inception_4c/3x3"\u000a  bottom: "inception_4c/5x5"\u000a  bottom: "inception_4c/pool_proj"\u000a  top: "inception_4c/output"\u000a}\u000a\u000alayer {\u000a  name: "inception_4d/1x1"\u000a  type: "Convolution"\u000a  bottom: "inception_4c/output"\u000a  top: "inception_4d/1x1"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 112\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4d/relu_1x1"\u000a  type: "ReLU"\u000a  bottom: "inception_4d/1x1"\u000a  top: "inception_4d/1x1"\u000a}\u000alayer {\u000a  name: "inception_4d/3x3_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_4c/output"\u000a  top: "inception_4d/3x3_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 144\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4d/relu_3x3_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_4d/3x3_reduce"\u000a  top: "inception_4d/3x3_reduce"\u000a}\u000alayer {\u000a  name: "inception_4d/3x3"\u000a  type: "Convolution"\u000a  bottom: "inception_4d/3x3_reduce"\u000a  top: "inception_4d/3x3"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 288\u000a    pad: 1\u000a    kernel_size: 3\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4d/relu_3x3"\u000a  type: "ReLU"\u000a  bottom: "inception_4d/3x3"\u000a  top: "inception_4d/3x3"\u000a}\u000alayer {\u000a  name: "inception_4d/5x5_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_4c/output"\u000a  top: "inception_4d/5x5_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 32\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4d/relu_5x5_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_4d/5x5_reduce"\u000a  top: "inception_4d/5x5_reduce"\u000a}\u000alayer {\u000a  name: "inception_4d/5x5"\u000a  type: "Convolution"\u000a  bottom: "inception_4d/5x5_reduce"\u000a  top: "inception_4d/5x5"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 64\u000a    pad: 2\u000a    kernel_size: 5\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4d/relu_5x5"\u000a  type: "ReLU"\u000a  bottom: "inception_4d/5x5"\u000a  top: "inception_4d/5x5"\u000a}\u000alayer {\u000a  name: "inception_4d/pool"\u000a  type: "Pooling"\u000a  bottom: "inception_4c/output"\u000a  top: "inception_4d/pool"\u000a  pooling_param {\u000a    pool: MAX\u000a    kernel_size: 3\u000a    stride: 1\u000a    pad: 1\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4d/pool_proj"\u000a  type: "Convolution"\u000a  bottom: "inception_4d/pool"\u000a  top: "inception_4d/pool_proj"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 64\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4d/relu_pool_proj"\u000a  type: "ReLU"\u000a  bottom: "inception_4d/pool_proj"\u000a  top: "inception_4d/pool_proj"\u000a}\u000alayer {\u000a  name: "inception_4d/output"\u000a  type: "Concat"\u000a  bottom: "inception_4d/1x1"\u000a  bottom: "inception_4d/3x3"\u000a  bottom: "inception_4d/5x5"\u000a  bottom: "inception_4d/pool_proj"\u000a  top: "inception_4d/output"\u000a}\u000a\u000alayer {\u000a  name: "inception_4e/1x1"\u000a  type: "Convolution"\u000a  bottom: "inception_4d/output"\u000a  top: "inception_4e/1x1"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 256\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4e/relu_1x1"\u000a  type: "ReLU"\u000a  bottom: "inception_4e/1x1"\u000a  top: "inception_4e/1x1"\u000a}\u000alayer {\u000a  name: "inception_4e/3x3_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_4d/output"\u000a  top: "inception_4e/3x3_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 160\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.09\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4e/relu_3x3_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_4e/3x3_reduce"\u000a  top: "inception_4e/3x3_reduce"\u000a}\u000alayer {\u000a  name: "inception_4e/3x3"\u000a  type: "Convolution"\u000a  bottom: "inception_4e/3x3_reduce"\u000a  top: "inception_4e/3x3"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 320\u000a    pad: 1\u000a    kernel_size: 3\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4e/relu_3x3"\u000a  type: "ReLU"\u000a  bottom: "inception_4e/3x3"\u000a  top: "inception_4e/3x3"\u000a}\u000alayer {\u000a  name: "inception_4e/5x5_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_4d/output"\u000a  top: "inception_4e/5x5_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 32\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.2\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4e/relu_5x5_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_4e/5x5_reduce"\u000a  top: "inception_4e/5x5_reduce"\u000a}\u000alayer {\u000a  name: "inception_4e/5x5"\u000a  type: "Convolution"\u000a  bottom: "inception_4e/5x5_reduce"\u000a  top: "inception_4e/5x5"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 128\u000a    pad: 2\u000a    kernel_size: 5\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4e/relu_5x5"\u000a  type: "ReLU"\u000a  bottom: "inception_4e/5x5"\u000a  top: "inception_4e/5x5"\u000a}\u000alayer {\u000a  name: "inception_4e/pool"\u000a  type: "Pooling"\u000a  bottom: "inception_4d/output"\u000a  top: "inception_4e/pool"\u000a  pooling_param {\u000a    pool: MAX\u000a    kernel_size: 3\u000a    stride: 1\u000a    pad: 1\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4e/pool_proj"\u000a  type: "Convolution"\u000a  bottom: "inception_4e/pool"\u000a  top: "inception_4e/pool_proj"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 128\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4e/relu_pool_proj"\u000a  type: "ReLU"\u000a  bottom: "inception_4e/pool_proj"\u000a  top: "inception_4e/pool_proj"\u000a}\u000alayer {\u000a  name: "inception_4e/output"\u000a  type: "Concat"\u000a  bottom: "inception_4e/1x1"\u000a  bottom: "inception_4e/3x3"\u000a  bottom: "inception_4e/5x5"\u000a  bottom: "inception_4e/pool_proj"\u000a  top: "inception_4e/output"\u000a}\u000a\u000a\u000a\u000alayer {\u000a  name: "inception_5a/1x1"\u000a  type: "Convolution"\u000a  bottom: "inception_4e/output"\u000a  top: "inception_5a/1x1"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 256\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5a/relu_1x1"\u000a  type: "ReLU"\u000a  bottom: "inception_5a/1x1"\u000a  top: "inception_5a/1x1"\u000a}\u000a\u000alayer {\u000a  name: "inception_5a/3x3_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_4e/output"\u000a  top: "inception_5a/3x3_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 160\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.09\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5a/relu_3x3_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_5a/3x3_reduce"\u000a  top: "inception_5a/3x3_reduce"\u000a}\u000a\u000alayer {\u000a  name: "inception_5a/3x3"\u000a  type: "Convolution"\u000a  bottom: "inception_5a/3x3_reduce"\u000a  top: "inception_5a/3x3"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 320\u000a    pad: 1\u000a    kernel_size: 3\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5a/relu_3x3"\u000a  type: "ReLU"\u000a  bottom: "inception_5a/3x3"\u000a  top: "inception_5a/3x3"\u000a}\u000alayer {\u000a  name: "inception_5a/5x5_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_4e/output"\u000a  top: "inception_5a/5x5_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 32\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.2\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5a/relu_5x5_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_5a/5x5_reduce"\u000a  top: "inception_5a/5x5_reduce"\u000a}\u000alayer {\u000a  name: "inception_5a/5x5"\u000a  type: "Convolution"\u000a  bottom: "inception_5a/5x5_reduce"\u000a  top: "inception_5a/5x5"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 128\u000a    pad: 2\u000a    kernel_size: 5\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5a/relu_5x5"\u000a  type: "ReLU"\u000a  bottom: "inception_5a/5x5"\u000a  top: "inception_5a/5x5"\u000a}\u000alayer {\u000a  name: "inception_5a/pool"\u000a  type: "Pooling"\u000a  bottom: "inception_4e/output"\u000a  top: "inception_5a/pool"\u000a  pooling_param {\u000a    pool: MAX\u000a    kernel_size: 3\u000a    stride: 1\u000a    pad: 1\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5a/pool_proj"\u000a  type: "Convolution"\u000a  bottom: "inception_5a/pool"\u000a  top: "inception_5a/pool_proj"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 128\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5a/relu_pool_proj"\u000a  type: "ReLU"\u000a  bottom: "inception_5a/pool_proj"\u000a  top: "inception_5a/pool_proj"\u000a}\u000alayer {\u000a  name: "inception_5a/output"\u000a  type: "Concat"\u000a  bottom: "inception_5a/1x1"\u000a  bottom: "inception_5a/3x3"\u000a  bottom: "inception_5a/5x5"\u000a  bottom: "inception_5a/pool_proj"\u000a  top: "inception_5a/output"\u000a}\u000a\u000alayer {\u000a  name: "inception_5b/1x1"\u000a  type: "Convolution"\u000a  bottom: "inception_5a/output"\u000a  top: "inception_5b/1x1"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 384\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5b/relu_1x1"\u000a  type: "ReLU"\u000a  bottom: "inception_5b/1x1"\u000a  top: "inception_5b/1x1"\u000a}\u000alayer {\u000a  name: "inception_5b/3x3_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_5a/output"\u000a  top: "inception_5b/3x3_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 192\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5b/relu_3x3_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_5b/3x3_reduce"\u000a  top: "inception_5b/3x3_reduce"\u000a}\u000alayer {\u000a  name: "inception_5b/3x3"\u000a  type: "Convolution"\u000a  bottom: "inception_5b/3x3_reduce"\u000a  top: "inception_5b/3x3"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 384\u000a    pad: 1\u000a    kernel_size: 3\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5b/relu_3x3"\u000a  type: "ReLU"\u000a  bottom: "inception_5b/3x3"\u000a  top: "inception_5b/3x3"\u000a}\u000alayer {\u000a  name: "inception_5b/5x5_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_5a/output"\u000a  top: "inception_5b/5x5_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 48\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5b/relu_5x5_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_5b/5x5_reduce"\u000a  top: "inception_5b/5x5_reduce"\u000a}\u000alayer {\u000a  name: "inception_5b/5x5"\u000a  type: "Convolution"\u000a  bottom: "inception_5b/5x5_reduce"\u000a  top: "inception_5b/5x5"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 128\u000a    pad: 2\u000a    kernel_size: 5\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5b/relu_5x5"\u000a  type: "ReLU"\u000a  bottom: "inception_5b/5x5"\u000a  top: "inception_5b/5x5"\u000a}\u000alayer {\u000a  name: "inception_5b/pool"\u000a  type: "Pooling"\u000a  bottom: "inception_5a/output"\u000a  top: "inception_5b/pool"\u000a  pooling_param {\u000a    pool: MAX\u000a    kernel_size: 3\u000a    stride: 1\u000a    pad: 1\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5b/pool_proj"\u000a  type: "Convolution"\u000a  bottom: "inception_5b/pool"\u000a  top: "inception_5b/pool_proj"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 128\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5b/relu_pool_proj"\u000a  type: "ReLU"\u000a  bottom: "inception_5b/pool_proj"\u000a  top: "inception_5b/pool_proj"\u000a}\u000alayer {\u000a  name: "inception_5b/output"\u000a  type: "Concat"\u000a  bottom: "inception_5b/1x1"\u000a  bottom: "inception_5b/3x3"\u000a  bottom: "inception_5b/5x5"\u000a  bottom: "inception_5b/pool_proj"\u000a  top: "inception_5b/output"\u000a}\u000alayer {\u000a  name: "pool5/drop_s1"\u000a  type: "Dropout"\u000a  bottom: "inception_5b/output"\u000a  top: "pool5/drop_s1"\u000a  dropout_param {\u000a    dropout_ratio: 0.4\u000a  }\u000a}\u000alayer {\u000a  name: "cvg/classifier"\u000a  type: "Convolution"\u000a  bottom: "pool5/drop_s1"\u000a  top: "cvg/classifier"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 1\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "coverage/sig"\u000a  type: "Sigmoid"\u000a  bottom: "cvg/classifier"\u000a  top: "coverage"\u000a}\u000alayer {\u000a  name: "bbox/regressor"\u000a  type: "Convolution"\u000a  bottom: "pool5/drop_s1"\u000a  top: "bboxes"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 4\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.\u000a    }\u000a  }\u000a}\u000a\u000a######################################################################\u000a# End of convolutional network\u000a######################################################################\u000a\u000a# Convert bboxes\u000alayer {\u000a  name: "bbox_mask"\u000a  type: "Eltwise"\u000a  bottom: "bboxes"\u000a  bottom: "coverage-block"\u000a  top: "bboxes-masked"\u000a  eltwise_param {\u000a    operation: PROD\u000a  }\u000a  include { phase: TRAIN }\u000a  include { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a  name: "bbox-norm"\u000a  type: "Eltwise"\u000a  bottom: "bboxes-masked"\u000a  bottom: "size-block"\u000a  top: "bboxes-masked-norm"\u000a  eltwise_param {\u000a    operation: PROD\u000a  }\u000a  include { phase: TRAIN }\u000a  include { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a  name: "bbox-obj-norm"\u000a  type: "Eltwise"\u000a  bottom: "bboxes-masked-norm"\u000a  bottom: "obj-block"\u000a  top: "bboxes-obj-masked-norm"\u000a  eltwise_param {\u000a    operation: PROD\u000a  }\u000a  include { phase: TRAIN }\u000a  include { phase: TEST stage: "val" }\u000a}\u000a\u000a# Loss layers\u000alayer {\u000a  name: "bbox_loss"\u000a  type: "L1Loss"\u000a  bottom: "bboxes-obj-masked-norm"\u000a  bottom: "bbox-obj-label-norm"\u000a  top: "loss_bbox"\u000a  loss_weight: 2\u000a  include { phase: TRAIN }\u000a  include { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a  name: "coverage_loss"\u000a  type: "EuclideanLoss"\u000a  bottom: "coverage"\u000a  bottom: "coverage-label"\u000a  top: "loss_coverage"\u000a  include { phase: TRAIN }\u000a  include { phase: TEST stage: "val" }\u000a}\u000a\u000a# Cluster bboxes\u000alayer {\u000a    type: 'Python'\u000a    name: 'cluster'\u000a    bottom: 'coverage'\u000a    bottom: 'bboxes'\u000a    top: 'bbox-list'\u000a    python_param {\u000a        module: 'caffe.layers.detectnet.clustering'\u000a        layer: 'ClusterDetections'\u000a        param_str : '640, 640, 16, 0.6, 2, 0.02, 22, 1'\u000a    }\u000a    include: { phase: TEST }\u000a}\u000a\u000a# Calculate mean average precision\u000alayer {\u000a  type: 'Python'\u000a  name: 'cluster_gt'\u000a  bottom: 'coverage-label'\u000a  bottom: 'bbox-label'\u000a  top: 'bbox-list-label'\u000a  python_param {\u000a      module: 'caffe.layers.detectnet.clustering'\u000a      layer: 'ClusterGroundtruth'\u000a      param_str : '640, 640, 16, 1'\u000a  }\u000a  include: { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a    type: 'Python'\u000a    name: 'score'\u000a    bottom: 'bbox-list-label'\u000a    bottom: 'bbox-list'\u000a    top: 'bbox-list-scored'\u000a    python_param {\u000a        module: 'caffe.layers.detectnet.mean_ap'\u000a        layer: 'ScoreDetections'\u000a    }\u000a    include: { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a    type: 'Python'\u000a    name: 'mAP'\u000a    bottom: 'bbox-list-scored'\u000a    top: 'mAP'\u000a    top: 'precision'\u000a    top: 'recall'\u000a    python_param {\u000a        module: 'caffe.layers.detectnet.mean_ap'\u000a        layer: 'mAP'\u000a        param_str : '640, 640, 16'\u000a    }\u000a    include: { phase: TEST stage: "val" }\u000a}\u000a
p245
sS'form.aug_hsv_v.data'
p246
F0.06
sS'form.solver_type.data'
p247
g200
sS'form.aug_noise.data'
p248
F0.0
sS'form.select_gpus_list.data'
p249
g233
sS'form.traces_interval.data'
p250
I0
sS'form.shuffle.data'
p251
I01
sS'form.aug_scale.data'
p252
F0.0
sS'form.batch_accumulation.data'
p253
NsS'form.batch_size.data'
p254
(lp255
NasS'form.lr_inv_power.data'
p256
F0.5
sS'form.group_name.data'
p257
g209
sS'form.lr_step_gamma.data'
p258
F0.1
sS'form.use_mean.data'
p259
g113
sS'form.lr_inv_gamma.data'
p260
F0.1
sS'form.aug_contrast.data'
p261
F0.0
sS'form.snapshot_interval.data'
p262
F10.0
sS'form.select_gpu.data'
p263
Vnext
p264
sS'form.lr_step_size.data'
p265
F33.0
sS'form.aug_hsv_use.data'
p266
I00
sS'form.aug_whitening.data'
p267
I00
sS'form.pretrained_networks.data'
p268
VNone
p269
sS'form.custom_network_snapshot.data'
p270
g98
sS'form.random_seed.data'
p271
NsS'form.lr_multistep_gamma.data'
p272
F0.5
sS'form.model_name.data'
p273
g206
sS'form.lr_exp_gamma.data'
p274
F0.97
sS'form.val_interval.data'
p275
F1.0
sS'form.aug_rot.data'
p276
I0
sS'form.train_epochs.data'
p277
I100
sS'form.lr_poly_power.data'
p278
F3.0
ssg136
F1.0
sS'_id'
p279
S'20181130-223734-9d03'
p280
sS'pickver_job_dataset'
p281
I1
sg174
(lp282
((idigits.status
Status
p283
g178
bF1543617454.739695
tp284
a((idigits.status
Status
p285
g181
bF1543617456.108738
tp286
a((idigits.status
Status
p287
g184
bF1543623598.027242
tp288
asb.